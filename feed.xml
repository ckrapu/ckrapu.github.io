<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ckrapu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ckrapu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-29T05:40:26+00:00</updated><id>https://ckrapu.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Solving climate change by abusing thermodynamic scaling laws</title><link href="https://ckrapu.github.io/blog/2024/why-dont-we-just-freeze-the-carbon/" rel="alternate" type="text/html" title="Solving climate change by abusing thermodynamic scaling laws"/><published>2024-09-28T10:00:00+00:00</published><updated>2024-09-28T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2024/why-dont-we-just-freeze-the-carbon</id><content type="html" xml:base="https://ckrapu.github.io/blog/2024/why-dont-we-just-freeze-the-carbon/"><![CDATA[<p><strong>TL;DR: To stop climate change, just do this - raise crops for biomass (sequestering CO2) and freeze them in huge aboveground piles during winter by running pipes through the middle. With a little insulation, they won’t ever thaw out during the summer. Since thermal exchange is \(\propto r^2\) and total heat capacity \(\propto r^3\), a big pile can be kept frozen indefinitely with a few tricks.</strong></p> <p>This is text from a research proposal I never finished. I keep thinking that I should do something with it. To keep from going crazy, I’m putting it here.</p> <h1 id="introduction">Introduction</h1> <p>The removal large quantities of carbon from Earth’s atmosphere is one of the most promising pathways to avoiding or mitigating the worst effects of global warming or climate change. Proposed methods for doing so include direct air capture (DAC) <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p> <p>An advantage of direct air capture is that the relevant infrastructure does not depend strongly on climate or soil type, and is limited primarily by manufacturing capacity and available energy. An alternative to using engineered systems for direct air capture is to instead allow plants to capture carbon via photosynthesis and store the vegetative biomass in a manner that prevents reentry of captured carbon back into circulation. Examples of biomass-centric carbon sequestration approaches include burial in deep, anoxic water <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, compression into polymer-lined bricks<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, carbon mineralization <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>, and storing salted biomass in a moisture-controlled environment <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. Unfortunately, storing carbon in deep water makes it challenging to reliably sense and monitor carbon emission rates. Carbon casting requires wrapping in polymer sheeting as well as storage in a controlled environment, thereby incurring substantial added costs while carbon mineralization requires extraction of substantial mineral resources.</p> <p>In approaches listed so far, my proposal is closest to the final option listed above as I also seek to take advantage of ambient environmental conditions to reduce biological activity in stored carbon, albeit making use of temperature instead of aridity. However, the approach advocated by Yablonovitch et al. requires the movement of very large masses of biological matter from productive agricultural regions to highly arid regions in which widespread cultivation is difficult without irrigation.</p> <p>Given the important role of C in Earth’s biosphere, it may be desirable in the future to access large repositories of accumulated C without causing substantial environmental damage. The aforementioned approaches would all require reprocessing of any stored biomass to render stored C biologically active and economically useful.</p> <p>I propose an alternative approach which sites C storage squarely within some of Earth’s most productive agricultural landscapes and without any special postharvest treatment such as drying, salting, or spraying with antimicrobial solution. I advocate taking advantage of large thermal gradients between summer and winter to freeze waterlogged piles of vegetation during the winter and rely on the system’s large thermal mass to keep the biomass biologically inactive during the summer. **More plainly, our design consists of a large pile of waterlogged biomass with pipes placed in parallel and with regular spacing running through the entirety of the mass. These pipes are open during winter and closed off during warmer conditions. An extra layer of dry biomass is added immediately before the onset of spring for added insulation. This process is repeated annually with additional biomass harvests with few additional constraints on the size of the design. Some important observations about the physical aspects of this design are that (1) thermal conductivities for waterlogged or ice-bound biomass are 10-40x greater than for dry insulating biomass, (2) the thermal exchange area of the design can be controlled via the installation of low-cost interior piping, (3) the latent heat of fusion for frozen water stored in the voids of the biomass provides large thermal inertia, and (4) natural processes of decomposition releasing carbon from biomass are slowed substantially at freezing temperatures and in anoxic conditions. To further understand whether using passive cooling during wintertime is a viable method of keeping biomass from decaying and emitting carbon, I address three primary questions:</p> <ol> <li> <p>Are estimated rates of C emission from biomass decay at near- or sub-freezing temperatures sufficiently low to enable carbon sequestration on timescales of a century or longer?</p> </li> <li> <p>What are the basic scaling relations between mass, size, and available thermal energy for a rudimentary storage design? 1</p> </li> <li> <p>Can the proposed approach be competitive with other approaches for cheaply sequestering carbon?</p> </li> </ol> <p>I conclude this proposal with a ranking of the most salient topics for further investigation and a plan of action for rapidly executing on a basic field trial for proof-of-concept.</p> <h2 id="biological-viability">Biological viability</h2> <p>(TL:DR; frozen carbon doesn’t decay very fast)</p> <p>Establishing bounds on the rate of carbon decay and emission from the proposed approach is challenging as most of the relevant literature on near-freezing respiration focuses on soil with a wide variety of microorganisms and varying proportions of inorganic material. Furthermore, many field studies focus on <em>areal</em> rates of respiration, summing over the entirety of the active soil column rather than volumetric rates. Despite these differences, a basic indication of the viability of this approach is the 1.7 billion tons of C estimated to be stored in permafrost soils currently; <sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup> provides a review of this accounting. Laboratory experiments conducted on arctic tundra soils obtain decay rates of 60 - 80 \(\mu\)g. C / g. C. / day at -0.5 C <sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>, approximately 3 - 10% of decay rates obtained at 14C. At even lower temperatures, <sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup> estimated that carbon emission rates of peatland soil at -14C. are approximately \(0.3 - 0.5\%\) of summer emission rates. The mechanisms for these emissions are not concretely identified. Competing explanations include a purely inorganic mechanism of freezing liquid forcing out pockets of carbon-bearing gas <sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup> as well as the creation of a transient dry front during freezing which provides suitable conditions for respiration <sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>. I also note existing research regarding the exhaustion of labile carbon stocks as a limit on low-temperature microbial respiration <sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup> <sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>. The findings of this work lend weight to the argument that laboratory experiments of carbon emissions from low-temperature biomass will substantially overestimate long-term rates as the former involve emissions from limited stocks of C which are quickly exhausted on monthly to annual timescales. Many of these studies focus on conditions favorable to aerobic respiration but the basic design I propose in this work would yield overwhelmingly anoxic conditions, potentially restricting respiration to be an order of magnitude lower than in aerobic conditions <sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">13</a></sup> <sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">14</a></sup>. While it is clear that both low-temperature and anoxic conditions each reduce microbial respiration rates on their own, additional review and experimental work is needed to predict respiration rates when both conditions apply. Another point of further analysis is the quantity of heat generated by metabolic mechanisms;<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">15</a></sup> additional modeling is needed to determine whether the heat produced by a small amount of reaction surface is sufficient to cause melting and lead to a positive feedback loop between biological activity and thawing.</p> <h2 id="thermal-modeling">Thermal modeling</h2> <p>(TL:DR; we can freeze a lot of harvested crops if the interior is exposed during the wintertime)</p> <p>In this section, I discuss the thermal energy budget for a highly simplified design, consisting of a biomass hemisphere of radius \(r\) resting on level ground. I further assume that all voids have been filled with water which has been frozen into solid ice. As a starting point for calculations, I assume that the hemisphere is of sufficient size to accommodate the harvest of one township (approx. 9300 hectares) of biomass cultivation over ten seasons at a conservative yield of approx. 3 tons per acre or 6800 kg / ha. Given these assumptions, \(6.3 \times 10^8\) kg of wet biomass could be harvested during that time. Assuming that the mass of wet biomass is double that of dry biomass, i.e. a water content of 50%, and a ratio of 1 to 1.8 for mass of dry biomass to O\(_{2}\)e, 5.8 Gg CO\(_2\)e could be contained within this site. For dry switchgrass biomass with a density of 125 kg/m\(^3\) <sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">16</a></sup>, this would occupy \(5.1\times 10^6\) cubic meters</p> <div style="text-align:center"> <img src="/assets/img/frozen-carbon-design-modified.png" alt="**Cross-sectional view of storage design**. The biomass is arranged in a hemisphere with radius r. To allow for thermal exchange during periods of low ambient temperature, conduits are placed at regular intervals of 2δ to increase heat transfer area."/> </div> <p>Suppose that we were to allow the voids of the hemisphere to be filled with frozen water and assume 50% porosity. I take this value on the basis of measurements reported for alfalfa bales <sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">17</a></sup>. The latent heat of fusion would be over \(7\times 10^{14}\) J, more than double the total annual solar irradiance (\(2.7\times 10^{14}\) J) for the area occupied by the bottom of the hemisphere. Next, we consider the ratio \(\gamma\) of interior thermal exchange surface area provided the by insides of the pipes to the exterior surface area. As the interior exchange area can be rendered largely inactive at will by plugging the openings of the pipes and thereby preventing air flow, this ratio serves as a measure of our ability to control thermal exchange such that there is increased heat flow out of the system during winter and reduced heat flow into the system during the summer. Assuming a hemispherical design with radius \(r\), the external area can be written as \(a_{ext} = 3\pi r^2\) while the total internal area of the pipe is \(a_{pipe}=\frac{4\pi r_p r^3}{3\delta^2}\) where \(r_p\) denotes the pipe radius. Then, the ratio between the two is \(\gamma = \frac{4r_p}{9\delta^2} r\) indicating that with a larger storage design, our ability to control thermal fluxes in and out of the system scales proportionally with \(r\). To provide an example using some rough numbers, letting \(r_p = 10\) cm, \(r=100\) m. and \(\delta=2\) m. yields \(\gamma \approx 1\). Each pipe would correspond to roughly \(\pi \delta^2\) square meters of cross-sectional area. However, allowing for larger spacing between pipes such that \(\gamma &lt;&lt; 1\) may be acceptable if the insulation for the exterior surface area is sufficiently thick.</p> <h3 id="internal-flow-rates">Internal flow rates</h3> <p>With regard to thermal exchange through piping installed throughout the mass, it is not clear under what combinations of pipe radius, pipe length, inlet size, and wind speeds are compatible with sufficient rates of heat exchange. As a rough estimate for flow in a pipe with Mach number \(M \approx 10^{-2}\), we can attempt to use Bernoulli’s equation for incompressible flow to approximate \(\Delta p=\frac{\rho}{2}\left(v_1^2 - v_2^2\right)\) where \(v_1, v_2\) denote wind speeds at opposite ends of the piping and \(\Delta p\) refers to the difference in pressure measured at each end of the pipe due to differences in wind speed. Then, the Hagen-Poiseuille equation predicts volumetric flow rate of \(Q=\frac{\pi^2 r_p^4 \Delta p}{8\mu r}\) where \(\mu\) is the dynamic viscosity of air and \(\rho\) is the density of air at 0 C. Plugging in reference values from earlier of \(r=100\) m., \(r_p=10^{-1}\) m., a difference in wind speeds of 5 meters per second, and \(\mu=1.8 \times 10^{-5}\) kilograms per meter-second, we arrive at a volumetric flow rate of \(Q = 2.2\) cubic meters per second<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">18</a></sup>, equivalent to a mass flow of 2.8 kg / s. Assuming operation during cold weather in which inflowing air is 5 K cooler than air at the pipe outlet, the heat exchanged from a single pipe in the storage mass and dissipated into the environment would constitute an energy flux of \(2.8 \times 5 \times 1006\) J/s or roughly \(1.4\times 10^4\) watts, providing enough exchange of thermal energy to freeze a cubic meter of liquid water at 0C in approximately six hours. Note that the thermal mass of the entire system is quite large; the latent heat of fusion for the water component assuming 50% porosity calculated previously as \(7\times 10^14\) J. Assuming continuous cooling for \(24\times 90 \times 3600\) seconds, nearly 6000 pipes would be needed to freeze all liquid water as desired. However, I note a few potential ways to work around this. First, the actual creation of the storage mound could be done in layers during wintertime conditions, allowing for the exterior surface area to contribute to heat dissipation as well. Second, the mound could be constructed over multiple winters, allowing for a much longer period of time to allow for the biomass-water mixture to be chilled and frozen.</p> <p>Further work is required to determine the ideal spacing and size of piping to attain the right balance of thermal exchange and cost.</p> <h3 id="external-insulation">External insulation</h3> <p>(TL;DR Thawing becomes really slow with even a modest layer of insulating plant matter)</p> <p>Measures to reduce the ambient environmental temperature within a buffer area around the storage site could significantly impact the overall thermal budget. These could include artificial insulation such as reflective covers or shades, or plantings of vegetation with high shade factor and/or high capacity for evapotranspiration to convert sensible heat into latent heat. The reference design presented so far is sufficiently tall (\(\approx 100\) m.) to render many of these expedients ineffective at shading the majority of the mass. A more economical choice may be to place a layer of dry vegetative matter on top of the design. Reports of R-values per inch for straw bales ranging from 0.5 to 2.5 have been reported <sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">19</a></sup> and it is likely that lightly packed, dry biomass would likely have similar insulating ability. Placing such a layer with thickness in excess of two meters would imply \(R \approx 80\). From another perspective, the thermal conductivity of straw bales has been reported at roughly \(5\times 10^{-2}\) watts per meter-degree <sup id="fnref:20" role="doc-noteref"><a href="#fn:20" class="footnote" rel="footnote">20</a></sup> while the same quantity for pure ice taken at the freezing point of water is nearly 45 times greater at \(2.2\) W / m-K <sup id="fnref:21" role="doc-noteref"><a href="#fn:21" class="footnote" rel="footnote">21</a></sup>. These statistics imply that thermal exchange through the exterior of the storage design would be substantially lower than through interior cooling conduits; the former consists of a modest amount of dry biomass layered on top while the interior is intended to be an ice-biomass mixture. However, layering dry matter on top does not come entirely without drawback; any biomass which is not frozen solid and exposed to the elements is likely to degrade more rapidly than similar mass contained deeper within the storage design. Determining an optimal insulating layer thickness and interior conduit spacing requires further investigation. Furthermore, it is unclear how much the value of this insulation would be reduced by precipitation falling onto the biomass. Given sufficient available water resources, one option for increasing the heat capacity of the storage design is to fill all voids in the biomass with water which may then freeze if exposed to cold weather. However, this reduces the naturally high insulative value of candidate biomass materials like switchgrass and miscanthus. It is likely that there exists some optimal strategy for partially filling the voids of the interior of the storage design while letting the outermost portions remain dry to provide insulation. Further work would be needed identify this optimum.</p> <h3 id="economic-viability">Economic viability</h3> <p>Showing feasibility for the proposed C storage design requires accounting for a variety of fixed and marginal costs. In this section, I reproduce several important numbers from <sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>, making use of their extensive economic analyses as a starting point for our own. As I similarly make use of agricultural biomass, I quote their numbers for cost per ton of CO\(_2\)e in USD of $6, $13, and $10 for land purchase, crop establishment, and post-establishment cultivation respectively for sequestered miscanthus. However, the design proposed in that work requires dry storage, application of electric dryers, and rain water protection which are unnecessary for our design, though we do also require operations with heavy equipment for stacking and placement of biomass as well as possible compression, yielding per-ton costs of $\(8.80\) under the assumptions of Yablonovitch et al. We also avoid most capital expenditures associated with the biolandfill design since no provision must be made for moisture management or geotechnical engineering. Altogether, the design elements which we share in common with that work constitute a per-CO\(_2\)e ton cost of approximately $38. This number omits (1) the cost of piping to allow for heat transfer, and (2) annual spring and fall adjustment of the pipe openings to allow or block heat exchange. For (1), I note that each linear meter of piping corresponds to \(\pi\delta^2 \approx 12.5\) cubic meters of stored biomass; at a dry density of 160 kg / m\(^3\), this implies a cost in piping material of $7.00 per ton CO\(_2\)e if using bulk polyvinyl chloride (PVC) pipe with \(r_p=0.1\) m. priced at $10.00 per meter. I anticipate negligible costs associated with manually sealing off or opening off the piping on a biannual basis. I also do not consider the net carbon emissions from manufacturing PVC piping. In total, I anticipate a total cost of USD $ 45 / ton CO\(_2\)e. At this price, As a final note, calculations associated with the simplified hemispherical design previously illustrated remain valid for arbitrarily large storage volumes. However, instead of increasing the radius of the hemisphere, it may be more expedient to extend the design along a horizontal axis to produce an oval-shaped or cylindrical repository.</p> <h2 id="next-steps">Next steps?</h2> <p>To see if this works, I propose some or any of the following</p> <ol> <li> <p>An experiment under controlled laboratory conditions measuring the metabolic activity of waterlogged biomass from several species including switchgrass and miscanthus, conducted over a range of temperatures, water content values, and vegetation fragment sizes.\</p> </li> <li> <p>A field trial using similar vegetative material at a relatively small scale (\(r \approx 10\)m.) with installed piping to verify key assumptions regarding insulation, heat capacity, and thaw rate</p> </li> <li> <p>Numerical simulations for calculating likely thermal exchange rates through different piping arrangements and densities</p> </li> <li> <p>Numerical simulations for a simplified system using representative forcing data for air temperature, incident solar radiation, and windspeed over 10+ years</p> </li> </ol> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>McQueen, N., Gomes, K. V., McCormick, C., Blumanthal, K., Pisciotta, M., &amp; Wilcox, J. (2021). A review of direct air capture (DAC): Scaling up commercial technologies and innovating for the future. Progress in Energy, 3(3), 032001. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>Keil, R. G., Nuwer, J. M., &amp; Strand, S. E. (2010). Burial of agricultural byproducts in the deep sea as a form of carbon sequestration: A preliminary experiment. Marine Chemistry, 122(1), 91-95. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>https://www.graphyte.com/ <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p>Gadikota, G. (2021). Carbon mineralization pathways for carbon capture, storage and utilization. Communications Chemistry, 4(1), 1-5. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5" role="doc-endnote"> <p>Yablonovitch, E., &amp; Deckman, H. W. (2023). Scalable, economical, and stable sequestration of agricultural fixed carbon. Proceedings of the National Academy of Sciences, 120(16), e2217695120. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:6" role="doc-endnote"> <p>Miner, K. R., Turetsky, M. R., Malina, E., Bartsch, A., Tamminen, J., McGuire, A. D., … &amp; Miller, C. E. (2022). Permafrost carbon emissions in a changing Arctic. Nature Reviews Earth &amp; Environment, 3(1), 55-67. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7" role="doc-endnote"> <p>Mikan, C. J., Schimel, J. P., &amp; Doyle, A. P. (2002). Temperature controls of microbial respiration in arctic tundra soils above and below freezing. Soil Biology and Biochemistry, 34(11), 1785-1795. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:8" role="doc-endnote"> <p>Panikov, N. S., &amp; Dedysh, S. N. (2000). Cold season CH4 and CO2 emission from boreal peat bogs (West Siberia): Winter fluxes and thaw activation dynamics. Global Biogeochemical Cycles, 14(4), 1071-1080. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9" role="doc-endnote"> <p>Coyne, P. I., &amp; Kelley, J. J. (1971). Release of carbon dioxide from frozen soil to the Arctic atmosphere. Nature, 234(5329), 407-408. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10" role="doc-endnote"> <p>Zimov, S. A., Zimova, G. M., Daviodov, S. P., Daviodova, A. I., Voropaev, Y. V., Voropaeva, Z. V., … &amp; Semiletov, I. P. (1993). Winter biotic activity and production of CO2 in Siberian soils: A factor in the greenhouse effect. Journal of Geophysical Research: Atmospheres, 98(D3), 5017-5023. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:11" role="doc-endnote"> <p>Schaefer, K., &amp; Jafarov, E. (2016). A parameterization of respiration in frozen soils based on substrate availability. Biogeosciences, 13(7), 1991-2001. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:12" role="doc-endnote"> <p>Sullivan, P. F., Stokes, M. C., McMillan, C. K., &amp; Weintraub, M. N. (2020). Labile carbon limits late winter microbial activity near Arctic treeline. Nature Communications, 11(1), 4024. <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:13" role="doc-endnote"> <p>Bridgham, S. D., Updegraff, K., &amp; Pastor, J. (1998). Carbon, nitrogen, and phosphorus mineralization in northern wetlands. Ecology, 79(5), 1545-1561. <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:14" role="doc-endnote"> <p>Schuur, E. A., Bockheim, J., Canadell, J. G., Euskirchen, E., Field, C. B., Goryachkin, S. V., … &amp; Zimov, S. A. (2008). Vulnerability of permafrost carbon to climate change: Implications for the global carbon cycle. BioScience, 58(8), 701-714. <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:15" role="doc-endnote"> <p>https://news.okstate.edu/articles/agriculture/2020/stotts_braums-fire.html <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:16" role="doc-endnote"> <p>Lanning, D. N., Dooley, J. H., Lanning, C. J., &amp; Fridley, J. L. (2014). U.S. Patent No. 8,757,368. Washington, DC: U.S. Patent and Trademark Office. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:17" role="doc-endnote"> <p>Yiljep, Y. D., Bilanski, W. K., &amp; Mittal, G. S. (1993). Porosity in large round bales of alfalfa herbage. Transactions of the ASAE, 36(6), 1791-1795. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:18" role="doc-endnote"> <p>Here, we have assumed that both pipe openings are at the same elevation. An additional corrective term of \(\rho g \left(y_2-y_1\right)\) could be included if the design is modified to allow for sloped piping. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:19" role="doc-endnote"> <p>Commins, T. R., &amp; Stone, N. I. (1998). Tested R-value for straw bale walls and performance modeling for straw bale homes. California Energy Commission. <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:20" role="doc-endnote"> <p>Costes, J. P., Evrard, A., Biot, B., Keutgen, G., Daras, A., Dubois, S., … &amp; Courard, L. (2017). Thermal conductivity of straw bales: Full size measurements considering the direction of the heat flow. Buildings, 7(1), 11. <a href="#fnref:20" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:21" role="doc-endnote"> <p>Huang, W., Li, Z., Liu, X., Zhao, H., Guo, S., &amp; Jia, Q. (2013). Effective thermal conductivity of reservoir freshwater ice with attention to high temperature. Annals of Glaciology, 54(62), 189-195. <a href="#fnref:21" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[A wintertime climate change solution]]></summary></entry><entry><title type="html">Modeling temporal data with an unknown number of changepoints</title><link href="https://ckrapu.github.io/blog/2022/nonparametric-changepoint-model-pymc/" rel="alternate" type="text/html" title="Modeling temporal data with an unknown number of changepoints"/><published>2022-11-20T15:12:00+00:00</published><updated>2022-11-20T15:12:00+00:00</updated><id>https://ckrapu.github.io/blog/2022/nonparametric-changepoint-model-pymc</id><content type="html" xml:base="https://ckrapu.github.io/blog/2022/nonparametric-changepoint-model-pymc/"><![CDATA[<p>Identifying structural breaks in data is an important problem to folks that frequently work with time series data. Some examples of how people have dealt with the problem of a single changepoint can be found <a href="https://cscherrer.github.io/post/bayesian-changepoint/">here</a> and <a href="https://mc-stan.org/docs/2_23/stan-users-guide/change-point-section.html">here</a>.</p> <p>Generally, the setup looks like this: we have some data \(X_t\) indexed by a discrete time coordinate \(t \in \{1,...,T\}\) and a parametric submodel linking the distribution of \(X\) to another quantity \(\mu_t\) which depends on the temporal coordinate. For the simple case of a linear Gaussian model with a single change point, we have</p> \[a_1, a_2 \sim N(0, \sigma^2_\mu)\] \[\tau \sim \text{DiscreteUniform}(\{1,...,T\})\] \[\mu_t = \left\{ \begin{array}{l} a_1 \text{ if } t &gt; \tau \\ a_2 \text{ if } t \le \tau \end{array} \right.\] \[X_t \sim N(\mu_t, \sigma_\epsilon)\] <p>with your scale priors of choice on the variance parameters \(\sigma_\epsilon\) and \(\sigma_\mu\). Now, one of the main conceptual problems with this model is that you need to assume it has a single changepoint. You can relax that assumption by extending this model to include more \(\tau\) and \(a\) parameters, but you’ll still need to specify the number of them ahead of time.</p> <p>Relaxing the assumption on the number of parameters is, for the most part, a solved problem in the research community (see <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167715297000503">here</a> and <a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1376&amp;context=statistics_papers">here</a> for a few representative examples). Unfortunately, these require the analyst to implement the inference techniques presented by hand; these are often Gibbs samplers or similar. Wouldn’t it be nice to just be able to use a PPL and write down the forward process instead?</p> <p>That’s the point of this notebook - we’ll walk through a construction of a changepoint model plus inference in PyMC which is considerably more straightforward than a handwritten sampler.</p> <p>We’ll start by simulating some data over 50 timesteps; there are 4 changepoints and the model’s likelihood is Gaussian. We will use a standard set of imports for working with PyMC and set the seed for repeatability.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">aesara.tensor</span> <span class="k">as</span> <span class="n">at</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">set_matplotlib_formats</span>
<span class="nf">set_matplotlib_formats</span><span class="p">(</span><span class="sh">'</span><span class="s">svg</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">pdf</span><span class="sh">'</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">827</span><span class="p">)</span>
</code></pre></div></div> <h1 id="simulating-a-dataset">Simulating a dataset</h1> <p>Since the generative process for this data is simple, the code required to simulate data is relatively short. We begin by sampling the changepoints and then adding offsets for each changepoint to the mean value of the data. We then perturb this mean with normal noise variates to create simulated observations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">noise_sd</span> <span class="o">=</span> <span class="mf">0.15</span>
<span class="n">n_changepoints</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">true_cp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_changepoints</span><span class="p">))</span>
<span class="n">offsets_per_period</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_changepoints</span><span class="p">)</span>

<span class="n">noiseless</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">changepoint</span><span class="p">,</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">true_cp</span><span class="p">,</span> <span class="n">offsets_per_period</span><span class="p">):</span>
  <span class="n">noiseless</span><span class="p">[</span><span class="n">start_time</span><span class="p">:</span><span class="n">changepoint</span><span class="p">]</span> <span class="o">+=</span> <span class="n">offset</span>
  <span class="n">start_time</span> <span class="o">=</span> <span class="n">changepoint</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">noiseless</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise_sd</span>
</code></pre></div></div> <p>As we can see below, the green changepoints do clearly correspond to changes in the level of the time series. However, not all of them are obvious - the last one, in particular, is a relatively small jump.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Observed data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">noiseless</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Noise-free mean value</span><span class="sh">'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cp</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">true_cp</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Change point</span><span class="sh">'</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">label</span><span class="o">=</span><span class="bp">None</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">cp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Timestep</span><span class="sh">'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">$$X(t)$$</span><span class="sh">'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/cp_data.svg" alt="svg"/></p> <h1 id="creating-the-model">Creating the model</h1> <p>For inference, we’ll assume that we don’t know the number of changepoints. The main trick that we’ll use is to instantiate way more changepoints than we need, and use latent variables to zero out most of them.</p> <p>The model that we declare looks like the following: \(p_{changepoint} \sim \text{Beta}(2,8)\)</p> \[\tau_1,...,\tau_{M} \sim \text{DiscreteUniform}(\{1,...,T\})\] \[u_1,...,u_M \sim \text{Bernoulli}(p_{changepoint})\] \[\mu \sim \text{Normal}(0, 1)\] \[\sigma^2_{\delta} \sim \text{HalfNormal}(2)\] \[\sigma^2_{\epsilon} \sim \text{HalfNormal}(1)\] \[\delta_1,...,\delta_M \sim \text{Normal}(0, \sigma^2_{\delta})\] <p>For convenience in our notation, we assume that \(\tau_1,...,\tau_M\) are ordered. We perform an elementwise multiplication of the \(\delta\) offsets with the latent binary variables \(u_m\) as well as indicator variables \(I\) \(\mu_t\):</p> \[\mu_t = \left[\left( \begin{array}{ccc} I(t\ge \tau_1) \\ \vdots \\ I(t\ge \tau_M) \end{array} \right) \odot \left( \begin{array}{ccc} \delta_1 \\ \vdots \\ \delta_M \end{array} \right)\right] \left(\begin{array}{ccc} u_1 \cdots u_M \end{array}\right)\] \[X_t \sim N(\mu_t, \sigma^2_\epsilon)\] <p>Since \(p_{changepoint}\) has a prior encouraging it to be lower, the indicator variables above will be pushed towards zero, thereby deactivating some of the \(\delta\) terms’ contributions towards \(X\).</p> <p>The code block below implements this model logic, though it uses <code class="language-plaintext highlighter-rouge">uniform_except_ends</code> to prevent any \(\tau\) values from occurring in the first two or last two timesteps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_cp_inference</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">tiled_times</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">repeat</span><span class="p">(</span><span class="n">max_cp_inference</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># We do this so that we can allow the Categorical prior over the changepoint
# locations to exclude the timesteps at the very beginning and very end. 
# The reason for this is that these data points always benefit from using an 
# extra changepoint just for the first or last data points. 
</span><span class="n">uniform_except_ends</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="n">uniform_except_ends</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">uniform_except_ends</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">uniform_except_ends</span> <span class="o">=</span> <span class="n">uniform_except_ends</span> <span class="o">/</span> <span class="n">uniform_except_ends</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
  <span class="c1"># Probability that any of the &lt;max_cp_inference&gt; change points are active
</span>  <span class="n">p_changepoint</span>  <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">p_changepoint</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

  <span class="c1"># Sort the changepoints for faster mixing / convergence
</span>  <span class="n">changepoints</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="sh">'</span><span class="s">changepoints</span><span class="sh">'</span><span class="p">,</span> <span class="n">uniform_except_ends</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">max_cp_inference</span><span class="p">)</span>
  <span class="n">is_cp_active</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Bernoulli</span><span class="p">(</span><span class="sh">'</span><span class="s">is_cp_active</span><span class="sh">'</span><span class="p">,</span> <span class="n">p_changepoint</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">max_cp_inference</span><span class="p">)</span>

  <span class="n">changepoints_sorted</span> <span class="o">=</span> <span class="n">at</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">changepoints</span><span class="p">)</span>

  <span class="c1"># This will give us a nice posterior estimate of the number of changepoints
</span>  <span class="n">num_active_cp</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Deterministic</span><span class="p">(</span><span class="sh">'</span><span class="s">num_active_cp</span><span class="sh">'</span><span class="p">,</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">is_cp_active</span><span class="p">))</span>

  <span class="n">global_mean</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">global_mean</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">cp_sd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">cp_sd</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">noise_sd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">noise_sd</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">changepoint_deltas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">changepoint_deltas</span><span class="sh">'</span><span class="p">,</span> <span class="n">cp_sd</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">max_cp_inference</span><span class="p">)</span>

  <span class="c1"># Operation involves operations on arrays with shape (T, max_cp_inference)
</span>  <span class="c1"># Elementwise operation zeros-out contributions from changepoints which are
</span>  <span class="c1"># not active
</span>  <span class="n">is_timestep_past_cp</span> <span class="o">=</span> <span class="p">(</span><span class="n">tiled_times</span> <span class="o">&gt;</span> <span class="n">changepoints</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:].</span><span class="nf">repeat</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="n">active_deltas</span> <span class="o">=</span> <span class="p">(</span><span class="n">changepoint_deltas</span><span class="o">*</span><span class="n">is_cp_active</span><span class="p">)</span>
  <span class="n">cp_contrib</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Deterministic</span><span class="p">(</span><span class="sh">'</span><span class="s">cp_contrib</span><span class="sh">'</span><span class="p">,</span>
                                <span class="n">global_mean</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">is_timestep_past_cp</span> <span class="o">*</span> <span class="n">active_deltas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="p">)</span>

  <span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">cp_contrib</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">noise_sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">xs</span><span class="p">)</span>

  <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

</code></pre></div></div> <style>progress{border:0;background-size:auto}progress:not([value]),progress:not([value])::-webkit-progress-bar{background:repeating-linear-gradient(45deg,#7e7e7e,#7e7e7e 10px,#5c5c5c 10px,#5c5c5c 20px)}.progress-bar-interrupted,.progress-bar-interrupted::-webkit-progress-bar{background:#f44336}</style> <div> <progress value="16000" class="" max="16000" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [16000/16000 02:30&lt;00:00 Sampling chain 0, 6,594 divergences] </div> <style>progress{border:0;background-size:auto}progress:not([value]),progress:not([value])::-webkit-progress-bar{background:repeating-linear-gradient(45deg,#7e7e7e,#7e7e7e 10px,#5c5c5c 10px,#5c5c5c 20px)}.progress-bar-interrupted,.progress-bar-interrupted::-webkit-progress-bar{background:#f44336}</style> <div> <progress value="16000" class="" max="16000" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [16000/16000 03:03&lt;00:00 Sampling chain 1, 3,278 divergences] </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERROR:pymc:There were 6594 divergences after tuning. Increase `target_accept` or reparameterize.
WARNING:pymc:The acceptance probability does not match the target. It is 0.08247, but should be close to 0.8. Try to increase the number of tuning steps.
ERROR:pymc:There were 9872 divergences after tuning. Increase `target_accept` or reparameterize.
WARNING:pymc:The acceptance probability does not match the target. It is 0.5083, but should be close to 0.8. Try to increase the number of tuning steps.
</code></pre></div></div> <p>From a sampling perspective, this is a pretty ugly problem. NUTS isn’t designed to work well in an alternating NUTS / Gibbs sampling scheme, and we get tons of divergences because NUTS is facing a log-posterior landscape that is shifting dramatically on every iteration because of the discrete latent variables.</p> <p>That said, the \(\hat{R}\) values look good - no warnings are fired off!</p> <h1 id="assessing-the-results-from-inference">Assessing the results from inference</h1> <p>As a basic statistic for the number of changepoints, we can just take the posterior mean of the indicator variables’ sum to see how many parameters were active, on average.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">num_active_cp</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <div><svg style="position: absolute; width: 0; height: 0; overflow: hidden"> <defs> <symbol id="icon-database" viewBox="0 0 32 32"> <path d="M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z"></path> <path d="M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path> <path d="M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path> </symbol> <symbol id="icon-file-text2" viewBox="0 0 32 32"> <path d="M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z"></path> <path d="M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path> <path d="M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path> <path d="M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path> </symbol> </defs> </svg> <style>:root{--xr-font-color0:var(--jp-content-font-color0,rgba(0,0,0,1));--xr-font-color2:var(--jp-content-font-color2,rgba(0,0,0,0.54));--xr-font-color3:var(--jp-content-font-color3,rgba(0,0,0,0.38));--xr-border-color:var(--jp-border-color2,#e0e0e0);--xr-disabled-color:var(--jp-layout-color3,#bdbdbd);--xr-background-color:var(--jp-layout-color0,white);--xr-background-color-row-even:var(--jp-layout-color1,white);--xr-background-color-row-odd:var(--jp-layout-color2,#eee)}html[theme=dark],body.vscode-dark{--xr-font-color0:rgba(255,255,255,1);--xr-font-color2:rgba(255,255,255,0.54);--xr-font-color3:rgba(255,255,255,0.38);--xr-border-color:#1f1f1f;--xr-disabled-color:#515151;--xr-background-color:#111;--xr-background-color-row-even:#111;--xr-background-color-row-odd:#313131}.xr-wrap{display:block!important;min-width:300px;max-width:700px}.xr-text-repr-fallback{display:none}.xr-header{padding-top:6px;padding-bottom:6px;margin-bottom:4px;border-bottom:solid 1px var(--xr-border-color)}.xr-header>div,.xr-header>ul{display:inline;margin-top:0;margin-bottom:0}.xr-obj-type,.xr-array-name{margin-left:2px;margin-right:10px}.xr-obj-type{color:var(--xr-font-color2)}.xr-sections{padding-left:0!important;display:grid;grid-template-columns:150px auto auto 1fr 20px 20px}.xr-section-item{display:contents}.xr-section-item input{display:none}.xr-section-item input+label{color:var(--xr-disabled-color)}.xr-section-item input:enabled+label{cursor:pointer;color:var(--xr-font-color2)}.xr-section-item input:enabled+label:hover{color:var(--xr-font-color0)}.xr-section-summary{grid-column:1;color:var(--xr-font-color2);font-weight:500}.xr-section-summary>span{display:inline-block;padding-left:.5em}.xr-section-summary-in:disabled+label{color:var(--xr-font-color2)}.xr-section-summary-in+label:before{display:inline-block;content:'►';font-size:11px;width:15px;text-align:center}.xr-section-summary-in:disabled+label:before{color:var(--xr-disabled-color)}.xr-section-summary-in:checked+label:before{content:'▼'}.xr-section-summary-in:checked+label>span{display:none}.xr-section-summary,.xr-section-inline-details{padding-top:4px;padding-bottom:4px}.xr-section-inline-details{grid-column:2 / -1}.xr-section-details{display:none;grid-column:1 / -1;margin-bottom:5px}.xr-section-summary-in:checked ~ .xr-section-details{display:contents}.xr-array-wrap{grid-column:1 / -1;display:grid;grid-template-columns:20px auto}.xr-array-wrap>label{grid-column:1;vertical-align:top}.xr-preview{color:var(--xr-font-color3)}.xr-array-preview,.xr-array-data{padding:0 5px!important;grid-column:2}.xr-array-data,.xr-array-in:checked ~ .xr-array-preview{display:none}.xr-array-in:checked ~ .xr-array-data,.xr-array-preview{display:inline-block}.xr-dim-list{display:inline-block!important;list-style:none;padding:0!important;margin:0}.xr-dim-list li{display:inline-block;padding:0;margin:0}.xr-dim-list:before{content:'('}.xr-dim-list:after{content:')'}.xr-dim-list li:not(:last-child):after{content:',';padding-right:5px}.xr-has-index{font-weight:bold}.xr-var-list,.xr-var-item{display:contents}.xr-var-item>div,.xr-var-item label,.xr-var-item>.xr-var-name span{background-color:var(--xr-background-color-row-even);margin-bottom:0}.xr-var-item>.xr-var-name:hover span{padding-right:5px}.xr-var-list>li:nth-child(odd)>div,.xr-var-list>li:nth-child(odd)>label,.xr-var-list>li:nth-child(odd)>.xr-var-name span{background-color:var(--xr-background-color-row-odd)}.xr-var-name{grid-column:1}.xr-var-dims{grid-column:2}.xr-var-dtype{grid-column:3;text-align:right;color:var(--xr-font-color2)}.xr-var-preview{grid-column:4}.xr-var-name,.xr-var-dims,.xr-var-dtype,.xr-preview,.xr-attrs dt{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;padding-right:10px}.xr-var-name:hover,.xr-var-dims:hover,.xr-var-dtype:hover,.xr-attrs dt:hover{overflow:visible;width:auto;z-index:1}.xr-var-attrs,.xr-var-data{display:none;background-color:var(--xr-background-color)!important;padding-bottom:5px!important}.xr-var-attrs-in:checked ~ .xr-var-attrs,.xr-var-data-in:checked ~ .xr-var-data{display:block}.xr-var-data>table{float:right}.xr-var-name span,.xr-var-data,.xr-attrs{padding-left:25px!important}.xr-attrs,.xr-var-attrs,.xr-var-data{grid-column:1 / -1}dl.xr-attrs{padding:0;margin:0;display:grid;grid-template-columns:125px auto}.xr-attrs dt,.xr-attrs dd{padding:0;margin:0;float:left;padding-right:10px;width:auto}.xr-attrs dt{font-weight:normal;grid-column:1}.xr-attrs dt:hover span{display:inline-block;background:var(--xr-background-color);padding-right:10px}.xr-attrs dd{grid-column:2;white-space:pre-wrap;word-break:break-all}.xr-icon-database,.xr-icon-file-text2{display:inline-block;vertical-align:middle;width:1em;height:1.5em!important;stroke-width:0;stroke:currentColor;fill:currentColor}</style><pre class="xr-text-repr-fallback">&lt;xarray.DataArray &#x27;num_active_cp&#x27; ()&gt;
array(3.2660625)</pre><div class="xr-wrap" style="display:none"><div class="xr-header"><div class="xr-obj-type">xarray.DataArray</div><div class="xr-array-name">'num_active_cp'</div></div><ul class="xr-sections"><li class="xr-section-item"><div class="xr-array-wrap"><input id="section-66376803-d2ba-411c-ba76-1595bd10deff" class="xr-array-in" type="checkbox" checked=""/><label for="section-66376803-d2ba-411c-ba76-1595bd10deff" title="Show/hide data repr"><svg class="icon xr-icon-database"><use xlink:href="#icon-database"></use></svg></label><div class="xr-array-preview xr-preview"><span>3.266</span></div><div class="xr-array-data"><pre>array(3.2660625)</pre></div></div></li><li class="xr-section-item"><input id="section-b11f32d2-7f05-4c59-92c9-35b91eea8d9e" class="xr-section-summary-in" type="checkbox" disabled=""/><label for="section-b11f32d2-7f05-4c59-92c9-35b91eea8d9e" class="xr-section-summary" title="Expand/collapse section">Coordinates: <span>(0)</span></label><div class="xr-section-inline-details"></div><div class="xr-section-details"><ul class="xr-var-list"></ul></div></li><li class="xr-section-item"><input id="section-7ea59420-61bc-4db6-b96c-ab2986f71ecb" class="xr-section-summary-in" type="checkbox" disabled=""/><label for="section-7ea59420-61bc-4db6-b96c-ab2986f71ecb" class="xr-section-summary" title="Expand/collapse section">Attributes: <span>(0)</span></label><div class="xr-section-inline-details"></div><div class="xr-section-details"><dl class="xr-attrs"></dl></div></li></ul></div></div> <p>We can also make a plot of the posterior inferences about the location and parameters of each changepoint as compared against the true values:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">top_10_cp</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span>
    <span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">changepoints</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">().</span><span class="nf">ravel</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()</span>
    <span class="p">).</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">noiseless</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True noiseless values</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">cp_contrib</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Inferred noiseless mean</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">)</span>

<span class="n">q10</span><span class="p">,</span> <span class="n">q90</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">cp_contrib</span><span class="sh">'</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">90</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">q10</span><span class="p">,</span> <span class="n">q90</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Observed data</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cp</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">true_cp</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">True change point</span><span class="sh">'</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">label</span><span class="o">=</span><span class="bp">None</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">cp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">top_10_cp</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Inferred change point</span><span class="sh">'</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">label</span><span class="o">=</span><span class="bp">None</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Timestep</span><span class="sh">'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">$$X(t)$$</span><span class="sh">'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">upper right</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/cp_inference.svg" alt="svg"/></p> <p>Here, the green vertical lines are the true changepoints while the orange vertical lines are one of the top 10 most likely changepoints as gleaned from the posterior samples. We can see that the major jumps around timesteps 10 and 20 are clearly captured, while there is more uncertainty from timesteps 20-40. The smaller jump at timestep 45 is also missed completely; this is not very surprising given how small it was.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A nonparametric changepoint model in PyMC]]></summary></entry><entry><title type="html">Distributed zonal averages for fast geospatial analyses</title><link href="https://ckrapu.github.io/blog/2022/fast-local-summary-earth-engine/" rel="alternate" type="text/html" title="Distributed zonal averages for fast geospatial analyses"/><published>2022-03-07T10:00:00+00:00</published><updated>2022-03-07T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2022/fast-local-summary-earth-engine</id><content type="html" xml:base="https://ckrapu.github.io/blog/2022/fast-local-summary-earth-engine/"><![CDATA[<p>Across many projects, I’ve needed to analyze remote sensing data and compute, for some points or polygons, a statistical summary of the remotely sensed layer in a neighborhood of those objects. Sometimes, that summary needs to be exactly calculated within the extent of the spatial object (like the boundaries of a field or a county) but other times, simply knowing the average in a circular or square buffer around the feature is good enough.</p> <p>I wrote this notebook to show what is the least painful way to do it for large numbers of geometries without having to manually retrieve and download data. We do this by making use of the functionality in Google’s Earth Engine via its Python API. In this example, I calculate the average amount of surface water within a 1 km. circular buffer around 100,000 points sampled within the vicinity of Washington, DC.</p> <p>The imports we require are fairly standard. I’ve used <code class="language-plaintext highlighter-rouge">contextily</code> here only to show a basemap comparison later on - you can omit this with no ill effect. Otherwise, we use the <code class="language-plaintext highlighter-rouge">ee</code> library for Earth Engine as well as Geopandas and the built-in <code class="language-plaintext highlighter-rouge">json</code> library.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">contextily</span> <span class="k">as</span> <span class="n">ctx</span>
<span class="kn">import</span> <span class="n">ee</span>
<span class="kn">import</span> <span class="n">geopandas</span>  <span class="k">as</span> <span class="n">gpd</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">numpy</span>      <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="n">shapely</span>   <span class="kn">import</span> <span class="n">geometry</span>
<span class="kn">from</span> <span class="n">time</span>      <span class="kn">import</span> <span class="n">time</span>
</code></pre></div></div> <p>The first thing we’ll do is create some fake data. Here, I randomly sample a large number of points as referenced by their latitude / longitude coordinates within a bounding box.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">left</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">upper</span><span class="o">=-</span><span class="mf">77.14</span><span class="p">,</span><span class="mf">38.81</span><span class="p">,</span><span class="o">-</span><span class="mf">76.90</span><span class="p">,</span><span class="mf">38.99</span>

<span class="n">n</span>     <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">longs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">lats</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">gdf</span>        <span class="o">=</span> <span class="n">gpd</span><span class="p">.</span><span class="nc">GeoDataFrame</span><span class="p">(</span><span class="n">geometry</span><span class="o">=</span><span class="p">[</span><span class="n">geometry</span><span class="p">.</span><span class="nc">Point</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">longs</span><span class="p">,</span><span class="n">lats</span><span class="p">)],</span> 
                <span class="n">crs</span><span class="o">=</span><span class="sh">'</span><span class="s">epsg:4326</span><span class="sh">'</span><span class="p">)</span>
<span class="n">n_geoms</span>    <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">gdf</span><span class="p">)</span>
</code></pre></div></div> <p>Next, we’ll indicate that the remote sensing image we want to average over is the Global Surface Water binary yes/no water layer which indicates, for each pixel, whether or not water was ever sensed in that pixel over the entire Landsat archive. We also define how big we want our local summary buffer to be, in terms of meters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image</span>         <span class="o">=</span> <span class="n">ee</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="sh">"</span><span class="s">JRC/GSW1_3/GlobalSurfaceWater</span><span class="sh">"</span><span class="p">).</span><span class="nf">select</span><span class="p">(</span><span class="sh">"</span><span class="s">max_extent</span><span class="sh">"</span><span class="p">)</span>
<span class="n">radius_meters</span> <span class="o">=</span> <span class="mi">1000</span>
</code></pre></div></div> <p>The most important part of this is the next code block. After instantiating an EarthEngine <code class="language-plaintext highlighter-rouge">FeatureCollection</code> corresponding to our points, we convolve the target image with a circular kernel around the points that we’ve supplied. We then ask Earth Engine to directly evaluate these summaries and then put them into our local memory.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">radial_average</span><span class="p">(</span><span class="n">ee_geoms</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">radius_meters</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Creates EE object for zonal average about each
    point in &lt;ee_geoms&gt; and directly evaluates it
    into the local kernel.
    </span><span class="sh">'''</span>

    <span class="n">kernel</span> <span class="o">=</span> <span class="n">ee</span><span class="p">.</span><span class="n">Kernel</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="n">radius_meters</span><span class="p">,</span>
                              <span class="n">units</span><span class="o">=</span><span class="sh">'</span><span class="s">meters</span><span class="sh">'</span><span class="p">,</span>
                              <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>

    <span class="n">smooth</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">convolve</span><span class="p">(</span><span class="n">kernel</span><span class="p">);</span>

    <span class="n">val_list</span> <span class="o">=</span> <span class="n">smooth</span><span class="p">.</span><span class="nf">reduceRegions</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
        <span class="sh">'</span><span class="s">collection</span><span class="sh">'</span><span class="p">:</span><span class="n">ee_geoms</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">reducer</span><span class="sh">'</span><span class="p">:</span><span class="n">ee</span><span class="p">.</span><span class="n">Reducer</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
    <span class="p">}).</span><span class="nf">aggregate_array</span><span class="p">(</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">val_array</span> <span class="o">=</span> <span class="n">ee</span><span class="p">.</span><span class="nc">Array</span><span class="p">(</span><span class="n">val_list</span><span class="p">).</span><span class="nf">toFloat</span><span class="p">().</span><span class="nf">getInfo</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">val_array</span>


</code></pre></div></div> <p>This loop takes our GeoDataFrame of points from earlier and splits it into smaller blocks so that we don’t hit the Earth Engine data transfer limit. We use the JSON representation of our GeoDataFrame to make it palatable to Earth Engine.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block_size</span>    <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_requests</span>    <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">n_geoms</span><span class="o">/</span><span class="n">block_size</span><span class="p">)</span>

<span class="n">blocks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_split</span><span class="p">(</span><span class="n">gdf</span><span class="p">,</span> <span class="n">n_requests</span><span class="p">)</span>

<span class="n">vals</span>  <span class="o">=</span> <span class="p">[]</span>
<span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>

<span class="n">ee</span><span class="p">.</span><span class="nc">Initialize</span><span class="p">()</span>

<span class="k">for</span> <span class="n">gdf_partial</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">:</span>
    <span class="n">js</span>           <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">gdf_partial</span><span class="p">.</span><span class="nf">to_json</span><span class="p">())</span>
    <span class="n">ee_geoms</span>     <span class="o">=</span> <span class="n">ee</span><span class="p">.</span><span class="nc">FeatureCollection</span><span class="p">(</span><span class="n">js</span><span class="p">)</span>
    <span class="n">partial_vals</span> <span class="o">=</span> <span class="nf">radial_average</span><span class="p">(</span><span class="n">ee_geoms</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">radius_meters</span><span class="p">)</span>
    
    <span class="n">vals</span> <span class="o">+=</span> <span class="p">[</span><span class="n">partial_vals</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>
    
<span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>8.036774396896362
7.84600043296814
7.808760643005371
6.694471120834351
8.156326532363892
7.155316114425659
7.938997268676758
6.8641228675842285
6.573741436004639
7.0107102394104
</code></pre></div></div> <p>Overall, it takes about 80 seconds for this to run. It could certainly be done faster locally with GDAL and/or zonalstats, but that requires extra setup and storing the data locally.</p> <p>To verify that our results are sensible, we overlay our point summaries with an OpenStreetMap-derived basemap for Washington DC.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">longs</span><span class="p">,</span><span class="n">lats</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">longs</span><span class="p">,</span><span class="n">lats</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="p">[</span><span class="n">ctx</span><span class="p">.</span><span class="nf">add_basemap</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="sh">'</span><span class="s">epsg:4326</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Proximity to water, per point</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">OSM basemap</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">();</span>
</code></pre></div></div> <p><img src="/images/dc_points.png" alt="png" style="width:75%;"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Easy local average with Google Earth Engine's Python API]]></summary></entry><entry><title type="html">Fast Kronecker matrix-vector product with einsum</title><link href="https://ckrapu.github.io/blog/2021/fast-matrix-vector-product-for-structured-matrices/" rel="alternate" type="text/html" title="Fast Kronecker matrix-vector product with einsum"/><published>2021-11-28T10:00:00+00:00</published><updated>2021-11-28T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2021/fast-matrix-vector-product-for-structured-matrices</id><content type="html" xml:base="https://ckrapu.github.io/blog/2021/fast-matrix-vector-product-for-structured-matrices/"><![CDATA[<p>In numerical linear algebra, a common problem that arises in the analysis of large datasets is the product of a dense but structured \(N \times N\) matrix \(\mathbf{A} = \bigotimes_{j=1}^J \mathbf{A}_j\) with a similarly dense vector \(\mathbf{y}\). We’re assuming that \(\mathbf{A}\) can be written as the tensor or Kronecker product of \(J\) smaller matrices denoted by \(\mathbf{A}_j\), each of which has dimension \(N_j\).</p> <p>Our strategy in computing this is to rearrange \(\mathbf{y}\) into a multdimensional array and, by contracting indices in an efficient way, avoid an \(\mathcal{O}(N^2)\) matrix-vector operation. Some of the commonly used identities of product matrices are available on <a href="https://en.wikipedia.org/wiki/Kronecker_product">Wikipedia</a>, and we’ll make use of several of them. We let \(\mathbf{Y}\) denote an array formed from the vector \(\mathbf{y}\) by reshaping into a form with axis dimensions of \(N_1,...,N_J\). By the associative property of the tensor product, \(\begin{align} \left(\bigotimes_{j=1}^J \mathbf{A}_j \right) \mathbf{y}&amp;=\mathbf{A}_1\otimes\left(\cdot\cdot\cdot\otimes(\mathbf{A}_J\mathbf{Y})\right)\\ &amp;=u^{(1)}_{k_1,k_2}u^{(2)}_{k_3,k_4}\cdot\cdot\cdot u^{(J)}_{k_{2J-1},k_{2J}}Y_{k_2,k_4,...,k_{2J}} \end{align}\) where we let \(u^{(1)}_{k_1k_2}\) refer to the entry of the \(k_1\)-th row and \(k_2\)-th column of \(\mathbf{A}_1\). The second equation above uses <a href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein notation</a> in representing the tensor product and multidimensional array \(\mathbf{Y}\). The \(k\) indices look a little funky compared to usual tensor notation; in physics we are used to having actual letters such as \(i, j, k\) rather than letters with numbers. However, if we have an arbitrary number of Kronecker factors, there may be many, many indices used, so we avoid using any particular letter and instead replace \(i, j, k, l, m,...\) with \(k_1, k_2, k_3, k_4, k_5,...\).</p> <p>The rule for Einstein notation is that when an index appears twice, we sum over it, also described as “contraction” over that index. Contracting the repeated indices, the result of the above procedure is an array \(\mathbf{Z}\) of the same dimensions as \(\mathbf{Y}\) running over indices \(k_1, k_3,...,k_{2J-1}\) which has been transformed by the repeated application of the matrix and tensor product operations and which satisfies the equality \(Vec(\mathbf{Z})=\left(\bigotimes_{j=1}^J \mathbf{A}_j \right) \mathbf{y}\). Since each of the \(J\) tensor contractions involves a sum involving \(N_j\) terms, each of which makes use of all \(N\) elements in \(\mathbf{Y}\), the overall complexity of this algorithm is \(\mathcal{O}(N \cdot \sum_{j=1}^J N_j)\), which compares favorably with the naive \(\mathcal{O}(N^2)\).</p> <p>This entire procedure can be run in a single call to the <code class="language-plaintext highlighter-rouge">einsum</code> function available in Numpy.</p> <p>Here, I should note that this is essentially the same result characterized by <a href="http://mlg.eng.cam.ac.uk/pub/pdf/Saa11.pdf">Saatchi’s PhD</a> thesis, but this presentation omits the dependence upon permutation indices and transpositions that obscures the essential index operations involved. Alex Williams has an <a href="https://gist.github.com/ahwillia/f65bc70cb30206d4eadec857b98c4065">implementation</a> of this calculation in PyTorch, but it does the index juggling by hand and is a bit more complex than automatically contracting the right indices. In the rest of this notebook, I show how to implement this operation in a few lines of Python.</p> <h3 id="implementing-an-efficient-matrix-vector-product">Implementing an efficient matrix-vector product</h3> <p>To begin, we’ll cook up a set of 5 square, symmetric matrices of increasing size. We’ll guarantee they are symmetic and positive semidefinite by squaring them.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">sizes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span>
<span class="n">prod_size</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>

<span class="n">matrices</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">]</span>
<span class="n">matrices</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="nd">@X.T</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">matrices</span><span class="p">]</span>


</code></pre></div></div> <p>In the end, we want to take the Kronecker / tensor product of these matrices. Since they have increasing dimension, the dimension of their Kronecker product will be <code class="language-plaintext highlighter-rouge">3*4*5*6*7=2520</code></p> <p>To see what the Kronecker product looks like, let’s see what the product of two of these matrices looks like:</p> <p><img src="/images/kron_im.png" alt="png" style="width:50%;"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="n">matrices</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[(3, 3), (4, 4), (5, 5), (6, 6), (7, 7)]
</code></pre></div></div> <p>As promised, these matrices are invertible as shown by their determinants. Since each of these determinants is nonzero, an inverse exists.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="n">matrices</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[19.055143537578502,
 0.041852313010475074,
 0.008158197604522445,
 43.252474950990084,
 798.5649833818011]
</code></pre></div></div> <p>We will also instantiate the vector \(\mathbf{y}\), though here we create it in the array form and then vectorize it later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">sizes</span><span class="p">)</span>
</code></pre></div></div> <p>Here, we perform a brute-force calculation of the matrix-vector product by instantiating the full Kronecker product. We do this by iteratively applying the Kronecker product to each of the matrices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>
<span class="n">big_matrix</span> <span class="o">=</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">kron</span><span class="p">,</span> <span class="n">matrices</span><span class="p">)</span>
<span class="n">matrix_product</span> <span class="o">=</span> <span class="n">big_matrix</span> <span class="o">@</span> <span class="n">y</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>
</code></pre></div></div> <p>We’ll also do the same using the <code class="language-plaintext highlighter-rouge">einsum</code> function. The first argument is a string specification for the tensor contraction. Essentially, it is saying that we have 5 two-dimensional arrays (with indices <code class="language-plaintext highlighter-rouge">ij</code>, <code class="language-plaintext highlighter-rouge">kl</code>, and so on), and that they are multiplied with a 5-dimensional array to output another 5-dimensional array.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensors</span> <span class="o">=</span> <span class="n">matrices</span><span class="o">+</span><span class="p">[</span><span class="n">y</span><span class="p">]</span>
<span class="n">einstein_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ij,kl,mn,op,qr,ikmoq-&gt;jlnpr</span><span class="sh">'</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">)</span>

</code></pre></div></div> <p>Both procedures result in the same values! Note that if you use the elementwise <code class="language-plaintext highlighter-rouge">==</code> operator overloaded by Numpy, you will get <code class="language-plaintext highlighter-rouge">False</code> due to minor differences due to the floating point representation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">matrix_product</span><span class="p">,</span> <span class="n">einstein_product</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div> <p>The next code cell packages up these functions so we can reuse them later to assess the relative runtimes of each.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">string</span> <span class="kn">import</span> <span class="n">ascii_lowercase</span> <span class="k">as</span> <span class="n">letters</span>

<span class="k">def</span> <span class="nf">mv_kron</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Compute product of vector and Kronecker-structured matrix
    via brute-force enumeration of entire Kronecker matrix.
    </span><span class="sh">'''</span>
    <span class="n">A_kron</span> <span class="o">=</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">kron</span><span class="p">,</span> <span class="n">matrices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">A_kron</span> <span class="o">@</span> <span class="n">y</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">mv_einstein</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Use Einstein summation convention to iteratively
    contract along secondary axes and implement Kronecker 
    matrix-vector product
    </span><span class="sh">'''</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">matrices</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">13</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">'</span><span class="s">There aren</span><span class="se">\'</span><span class="s">t enough letters in the alphabet for this operation :(</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="n">letter_pairs</span> <span class="o">=</span> <span class="p">[</span><span class="n">letters</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">letters</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span>
    <span class="n">matrix_string</span> <span class="o">=</span> <span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">letter_pairs</span><span class="p">)</span>
    <span class="n">vec_in_string</span><span class="p">,</span> <span class="n">vec_out_string</span> <span class="o">=</span> <span class="p">[</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">letter_pairs</span><span class="p">)]</span>
    <span class="n">string_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">matrix_string</span><span class="si">}</span><span class="s">,</span><span class="si">{</span><span class="n">vec_in_string</span><span class="si">}</span><span class="s">-&gt;</span><span class="si">{</span><span class="n">vec_out_string</span><span class="si">}</span><span class="sh">'</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">string_spec</span><span class="p">,</span> <span class="o">*</span><span class="n">matrices</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="sh">'</span><span class="s">greedy</span><span class="sh">'</span><span class="p">).</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">string_spec</span>
</code></pre></div></div> <h3 id="comparing-runtimes">Comparing runtimes</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ein_times</span>  <span class="o">=</span> <span class="p">[]</span>
<span class="n">kron_times</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dimensions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="mi">8</span><span class="o">*</span><span class="n">scale</span><span class="p">]</span>
    <span class="n">matrices</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">]</span>
    <span class="n">matrices</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="nd">@X.T</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">matrices</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">sizes</span><span class="p">)</span>
    
    <span class="n">start_kron</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="nf">mv_kron</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">end_kron</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_kron</span>
    
    <span class="n">start_ein</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">string_spec</span> <span class="o">=</span> <span class="nf">mv_einstein</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">end_ein</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_ein</span>
    
    <span class="n">ein_times</span>  <span class="o">+=</span> <span class="p">[</span><span class="n">end_ein</span><span class="p">]</span>
    <span class="n">kron_times</span> <span class="o">+=</span> <span class="p">[</span><span class="n">end_kron</span><span class="p">]</span>
    <span class="n">dimensions</span> <span class="o">+=</span> <span class="p">[</span><span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Dimension:</span><span class="sh">'</span><span class="p">,</span> <span class="nf">str</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">).</span><span class="nf">ljust</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="sa">f</span><span class="sh">'</span><span class="s">Einstein time: </span><span class="si">{</span><span class="n">end_ein</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> s.</span><span class="sh">'</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">Naive time </span><span class="si">{</span><span class="n">end_kron</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s.</span><span class="sh">'</span><span class="p">)</span>


</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dimension: 64    Einstein time: 0.001 s. Naive time 0.001s.
Dimension: 512   Einstein time: 0.000 s. Naive time 0.016s.
Dimension: 1728  Einstein time: 0.000 s. Naive time 0.039s.
Dimension: 4096  Einstein time: 0.000 s. Naive time 0.190s.
Dimension: 8000  Einstein time: 0.001 s. Naive time 0.921s.
Dimension: 13824 Einstein time: 0.001 s. Naive time 2.632s.
Dimension: 21952 Einstein time: 0.018 s. Naive time 7.073s.
Dimension: 32768 Einstein time: 0.063 s. Naive time 95.083s.
</code></pre></div></div> <p>As we can see below, there’s a big disparity in runtime, although I can’t really probe any larger dimension sizes since my laptop has a small amount of memory.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">ein_times</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Einstein</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">kron_times</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">m</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Naive</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">$\log_{10}$ runtime</span><span class="sh">'</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">$N$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">../figures/kmvp_runtime.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>

</code></pre></div></div> <p><img src="/images/kmvp_runtime.png" alt="png" style="width:75%;"/></p> <p>Finally, it’s interesting to note that behind-the-scenes, the Numpy implementation of <code class="language-plaintext highlighter-rouge">einsum</code> is performing a <em>path optimization</em> to determine which indices should be contracted first. We can check it out by calling <code class="language-plaintext highlighter-rouge">einsum_path</code> and examining the results.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="nf">print</span><span class="p">(</span><span class="n">string</span><span class="p">)</span> <span class="k">for</span> <span class="n">string</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum_path</span><span class="p">(</span><span class="n">string_spec</span><span class="p">,</span> <span class="o">*</span><span class="n">matrices</span><span class="p">,</span> <span class="n">y</span><span class="p">)];</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['einsum_path', (2, 3), (1, 2), (0, 1)]
  Complete contraction:  ab,cd,ef,ace-&gt;bdf
         Naive scaling:  6
     Optimized scaling:  4
      Naive FLOP count:  4.295e+09
  Optimized FLOP count:  7.340e+06
   Theoretical speedup:  585.143
  Largest intermediate:  3.277e+04 elements
--------------------------------------------------------------------------
scaling                  current                                remaining
--------------------------------------------------------------------------
   4                 ace,ef-&gt;acf                           ab,cd,acf-&gt;bdf
   4                 acf,cd-&gt;adf                              ab,adf-&gt;bdf
   4                 adf,ab-&gt;bdf                                 bdf-&gt;bdf
</code></pre></div></div> <p>This printout tells us two things: first, the number of floating point operations is nearly 600X smaller for the optimized summation path. Second, the largest array held in memory is only on the order of \(10^4\) elements, so it’s much more memory efficient.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Easy local average with Google Earth Engine's Python API]]></summary></entry><entry><title type="html">Balanced spatial partitioning for point data in 20 lines</title><link href="https://ckrapu.github.io/blog/2021/balanced-spatial-partitioning-for-point-data-in-20-lines/" rel="alternate" type="text/html" title="Balanced spatial partitioning for point data in 20 lines"/><published>2021-11-10T10:00:00+00:00</published><updated>2021-11-10T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2021/balanced-spatial-partitioning-for-point-data-in-20-lines</id><content type="html" xml:base="https://ckrapu.github.io/blog/2021/balanced-spatial-partitioning-for-point-data-in-20-lines/"><![CDATA[<p>When working with geospatial data, sometimes a dataset is simply too large in its original form or file to be worked with effectively. I often work with hierarchical statistical models that function effectively when a larger dataset is partitioned into smaller subsets. To preprocess the data, I frequently need to find a way to split up a larger spatial domain into smaller pieces such that the number of objects or data points in each piece is roughly equal.</p> <p>Unfortunately, I couldn’t find a quick reference online to do this with Python, so this post covers how to do it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">geopandas</span>         <span class="k">as</span> <span class="n">gpd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="sh">'</span><span class="s">retina</span><span class="sh">'</span>
<span class="n">load_path</span> <span class="o">=</span><span class="sh">'</span><span class="s">../sample_data.gpkg</span><span class="sh">'</span>
<span class="n">gdf</span>       <span class="o">=</span> <span class="n">gpd</span><span class="p">.</span><span class="nf">read_file</span><span class="p">(</span><span class="n">load_path</span><span class="p">).</span><span class="nf">to_crs</span><span class="p">(</span><span class="sh">'</span><span class="s">epsg:2283</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The data we’ll be working with is a set of building footprints for structures in Washington, DC. These have been collected from OpenStreetMap, and there’s roughly 134,000 of them:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">gdf</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>134846
</code></pre></div></div> <p>As we can see from the plot below, these buildings aren’t already nicely spaced into even subsets. There’s many more in the urban core of Washington, DC. Also, the geometry of the region itself is somewhat irregular.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gdf</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/dc_buildings_blue.png" alt="png" style="width:75%;"/></p> <p>To split up the data, we’re going to use a recursive approach. The main data structure we’re working is going to be a list of tuples containing an x-coordinate, y-coordinate, and feature index, respectively. We will recursively split subsets of features into balanced north-south or east-west halves by bisecting a sorted array. If we denote the number of splits as \(M\) and the number of features as \(N\), this naive approach has a complexity of \(\mathcal{O}(MN \log N)\) and thus scales well to large-ish datasets. We shouldn’t need to sort at each splitting, however, so really, this algorithm should be running in \(\mathcal{O}(M+N \log N)\) time. I didn’t take the time to make that modification here since the original version was fast enough.</p> <p>The top-level function (shown below) initializes the required variables and also post-processes the subsets by repeatedly flattening a nested list-of-lists until only the bottom-level results of the recursion are contained in a single top-level list.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">gdf</span><span class="p">,</span> <span class="n">max_level</span><span class="p">):</span>
    
    <span class="n">xs</span> <span class="o">=</span> <span class="p">(</span><span class="n">gdf</span><span class="p">.</span><span class="n">centroid</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">gdf</span><span class="p">.</span><span class="n">centroid</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">gdf</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">xs</span><span class="p">))</span>
    
    <span class="n">splits</span> <span class="o">=</span> <span class="nf">split_recurse</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_level</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_level</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="p">[])</span>
        
    <span class="n">indices_only</span> <span class="o">=</span> <span class="p">[[</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">]</span> <span class="k">for</span> <span class="n">subset</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">]</span>
        
    <span class="k">return</span> <span class="p">[</span><span class="n">gdf</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">indices_only</span><span class="p">]</span>
</code></pre></div></div> <p>The recursive function is defined below. It’s pretty simple - we just sort, split, and continue on with each subset. The parameter <code class="language-plaintext highlighter-rouge">max_level</code> controls how many partition cells there are; we do a binary split at each level, resulting in <code class="language-plaintext highlighter-rouge">2**max_level</code> cells by the time we’re finished.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split_recurse</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">split_pos</span><span class="p">,</span> <span class="n">max_level</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    
    <span class="n">xs</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="n">split_pos</span><span class="p">])</span>
    <span class="n">mid</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">above</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">mid</span><span class="p">]</span>
    <span class="n">below</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">mid</span><span class="p">]</span>
    
    <span class="n">subsets</span> <span class="o">=</span> <span class="p">[</span><span class="n">above</span><span class="p">,</span> <span class="n">below</span><span class="p">]</span>
        
    <span class="k">if</span> <span class="n">level</span> <span class="o">==</span> <span class="n">max_level</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">subsets</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We flip between using the 0-th position and the 1st position in 
</span>        <span class="c1"># our triplets to alternate between x- and y-coordinates for splitting.
</span>        <span class="k">return</span> <span class="p">[</span><span class="nf">split_recurse</span><span class="p">(</span><span class="n">subset</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">split_pos</span><span class="p">,</span> <span class="n">max_level</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">level</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">subset</span> <span class="ow">in</span> <span class="n">subsets</span><span class="p">]</span>
</code></pre></div></div> <p>Let’s see how the results are distributed in space. The plot below shows the assignment of each building to a partition cell; each color is a different cell.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">color_string</span> <span class="o">=</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">bgrcmykbgrcmykbgrcmykbgrcmyk</span><span class="sh">'</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">gdf_splits</span> <span class="o">=</span> <span class="nf">split</span><span class="p">(</span><span class="n">gdf</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="p">[</span><span class="n">subset</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span> <span class="k">for</span> <span class="n">color</span><span class="p">,</span> <span class="n">subset</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">color_string</span><span class="p">,</span> <span class="n">gdf_splits</span><span class="p">)];</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/dc_buildings_rainbow.png" alt="png" style="width:75%;"/></p> <p>We can also see whether or not the subsets are balanced in size:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="k">for</span> <span class="n">subset</span> <span class="ow">in</span> <span class="n">gdf_splits</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[4212,
 4214,
 4213,
 4215,
 ...
 4214,
 4215,
 4213,
 4215,
 4214,
 4215]
</code></pre></div></div> <p>All of the cells have nearly the same number of points between them!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Recursively splitting by boxes]]></summary></entry><entry><title type="html">Modeling spatial structure in binary data with an H3 hexagonal coordinate system</title><link href="https://ckrapu.github.io/blog/2021/clustering-for-prob-preterm-birth/" rel="alternate" type="text/html" title="Modeling spatial structure in binary data with an H3 hexagonal coordinate system"/><published>2021-04-29T10:00:00+00:00</published><updated>2021-04-29T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2021/clustering-for-prob-preterm-birth</id><content type="html" xml:base="https://ckrapu.github.io/blog/2021/clustering-for-prob-preterm-birth/"><![CDATA[<p>We often model geostatistical (i.e. point-referenced data) in order to determine whether or not there are spatial patterns of autocorrelation. The object of interest is frequently an underlying spatial function giving rise to patterns of spatially correlated data. When we work with discrete observational data, a problem arises - we want to study smoothly-varying response surfaces over space, but the data themselves are not continuous and therefore we cannot specify a likelihood which is continuous in both space and response. Consequently, we often choose to <strong>reparameterize</strong> our model in terms of a latent smooth spatial surface and a link function mapping this spatial surface to the parameters of a likelihood function appropriate for discrete data.</p> <p>This notebook shows how to analyze binary geospatial point data using a spatially-smoothing conditional autoregression model to test for the existence of clusters of 0 or 1 values. The dataset used in this example is <strong>simulated</strong> data of preterm births in Washington, DC. While many autoregressive models use square grids, we’re going to use a hexagonal tiling from Uber’s H3 coordinate system library to demarcate our areal units.</p> <h2 id="generating-simulated-data">Generating simulated data</h2> <p>We begin by importing the requisite libraries and simulating synthetic data of preterm births.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">geopandas</span> <span class="k">as</span> <span class="n">gpd</span>
<span class="kn">import</span> <span class="n">h3</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">networkx</span> <span class="k">as</span> <span class="n">nx</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pymc3</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">shapely</span>

<span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">BallTree</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span><span class="o">=</span><span class="sh">'</span><span class="s">retina</span><span class="sh">'</span>
</code></pre></div></div> <p>To ensure reproducibility, it’s a good habit to include a version stamp as shown below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
<span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">iv</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The watermark extension is already loaded. To reload it, use:
  %reload_ext watermark
h3        : 3.7.2
pymc3     : 3.11.1
shapely   : 1.7.1
geopandas : 0.8.1
numpy     : 1.18.5
pandas    : 1.1.3
matplotlib: 3.3.2
networkx  : 2.5
</code></pre></div></div> <p>At several points we will need to use multiple functions to handle geospatial operations such as creating point data, determining the adjacency of vector features, and ensuring that sets of spatial objects are topologically connected. We’ll define them now so we can use them later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">xy_from_gdf</span><span class="p">(</span><span class="n">gdf</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Returns Nx2 matrix of X,Y coordinates from a GeoDataFrame
    </span><span class="sh">'''</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">gdf</span><span class="p">.</span><span class="n">centroid</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">gdf</span><span class="p">.</span><span class="n">centroid</span><span class="p">.</span><span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">lat_lng_to_h3</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="n">h3_level</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Applies H3</span><span class="sh">'</span><span class="s">s geocoding to determine the hexagonal cell
    containing a given point. The h3 level determines the
    size of the hexagonal lattice used.
    </span><span class="sh">'''</span>
    <span class="k">return</span> <span class="n">h3</span><span class="p">.</span><span class="nf">geo_to_h3</span><span class="p">(</span>
      <span class="n">point</span><span class="p">.</span><span class="n">geometry</span><span class="p">.</span><span class="n">centroid</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">point</span><span class="p">.</span><span class="n">geometry</span><span class="p">.</span><span class="n">centroid</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">h3_level</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_geometry</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Creates a vector feature from the H3 hexagonal coordinates.
    </span><span class="sh">'''</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">h3</span><span class="p">.</span><span class="nf">h3_to_geo_boundary</span><span class="p">(</span>
      <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">],</span> <span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shapely</span><span class="p">.</span><span class="n">geometry</span><span class="p">.</span><span class="nc">Polygon</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">nearest_neighbor_centroid</span><span class="p">(</span><span class="n">gdf1</span><span class="p">,</span> <span class="n">gdf2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Vectorized operation for identifying the nearest points in gdf2 relative to gdf1.
    </span><span class="sh">'''</span>
    <span class="n">X_proposed</span><span class="p">,</span> <span class="n">X_base</span> <span class="o">=</span> <span class="nf">xy_from_gdf</span><span class="p">(</span><span class="n">gdf1</span><span class="p">),</span> <span class="nf">xy_from_gdf</span><span class="p">(</span><span class="n">gdf2</span><span class="p">)</span>        
    <span class="n">nearest</span> <span class="o">=</span> <span class="nc">BallTree</span><span class="p">(</span><span class="n">X_proposed</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="nf">query</span><span class="p">(</span><span class="n">X_base</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nearest</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">adjacency_via_buffer</span><span class="p">(</span><span class="n">gdf</span><span class="p">,</span> <span class="n">very_small_distance</span><span class="o">=</span><span class="mf">0.0003</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Uses a spatial buffering and intersection operator to determine
    which features share a boundary in a GeoDataFrame.
    </span><span class="sh">'''</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">gdf</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="n">buffered</span> <span class="o">=</span> <span class="n">gdf</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">buffered</span><span class="p">.</span><span class="n">geometry</span> <span class="o">=</span> <span class="n">buffered</span><span class="p">.</span><span class="nf">buffer</span><span class="p">(</span><span class="n">very_small_distance</span><span class="p">)</span>

    <span class="c1"># Find neighbors by buffering and locating non-null overlap
</span>    <span class="n">nearby</span>  <span class="o">=</span> <span class="n">buffered</span><span class="p">.</span><span class="n">geometry</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">buffered</span><span class="p">.</span><span class="nf">intersection</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">area</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">nearest</span> <span class="o">=</span> <span class="nf">nearest_neighbor_centroid</span><span class="p">(</span><span class="n">gdf</span><span class="p">,</span> <span class="n">gdf</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">:]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">neighbors</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">nearby</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>          <span class="o">-=</span> <span class="mi">1</span> <span class="c1"># self-neighboring is not allowed
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">nearest</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">+</span><span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">&gt;</span> <span class="mf">0.</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W</span>

<span class="k">def</span> <span class="nf">connect_components</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">geom_series</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Iteratively add edges between nodes to the network
    until only a single edge-connected component covers the entire graph. This
    is critical for usage of the CAR model, which can fail if there
    are </span><span class="sh">"</span><span class="s">islands</span><span class="sh">"</span><span class="s"> disconnected from each other in the network / adjacency matrix.
    </span><span class="sh">'''</span>
    <span class="n">connected</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">connected</span><span class="p">:</span>

        <span class="c1"># Find the largest component and drop it from
</span>        <span class="c1"># the list of islands
</span>        <span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="n">convert_matrix</span><span class="p">.</span><span class="nf">from_numpy_matrix</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">nx</span><span class="p">.</span><span class="nf">is_connected</span><span class="p">(</span><span class="n">G</span><span class="p">):</span>
            <span class="k">break</span>
            
        <span class="n">components</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">nx</span><span class="p">.</span><span class="nf">connected_components</span><span class="p">(</span><span class="n">G</span><span class="p">))</span>
        <span class="n">sizes</span>      <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">components</span><span class="p">]</span>
        <span class="n">largest</span>    <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
        <span class="n">components</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="n">largest</span><span class="p">)</span>

        <span class="c1"># For each island, find the nearest node not on
</span>        <span class="c1"># the island and hook it up
</span>        <span class="k">for</span> <span class="n">island</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
            <span class="n">element_on_island</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">island</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">geom_on_island</span>    <span class="o">=</span> <span class="n">geom_series</span><span class="p">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">element_on_island</span><span class="p">]]</span>
            <span class="n">repeated</span>  <span class="o">=</span> <span class="n">geom_on_island</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">geom_series</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">geom_series</span><span class="p">.</span><span class="n">geometry</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">geom_on_island</span><span class="p">.</span><span class="nf">distance</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="n">ordered_by_dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>

            <span class="n">connected_for_island</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">ctr</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">connected_for_island</span><span class="p">:</span>
                <span class="n">proposed</span> <span class="o">=</span> <span class="n">ordered_by_dist</span><span class="p">[</span><span class="n">ctr</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">proposed</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">island</span><span class="p">:</span>
                    <span class="n">W</span><span class="p">[</span><span class="n">element_on_island</span><span class="p">,</span> <span class="n">proposed</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="n">W</span><span class="p">[</span><span class="n">proposed</span><span class="p">,</span> <span class="n">element_on_island</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Match for element {0} is {1}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">element_on_island</span><span class="p">,</span> <span class="n">proposed</span><span class="p">))</span>
                    <span class="n">connected_for_island</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="n">ctr</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="n">convert_matrix</span><span class="p">.</span><span class="nf">from_numpy_matrix</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">connected</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="nf">is_connected</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W</span>

</code></pre></div></div> <p>Next, we use a shapefile of census tract data to determine how to sample birth events over space. We will use the population within each census tract, combined with a national average birth rate to determine how many births will be placed within each tract.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">'''</span><span class="s">
Census tract shapefile taken from https://opendata.arcgis.com/datasets/f33d847161174e81ad59c9ea9c1f5a00_36.zip
</span><span class="sh">'''</span>
<span class="n">census_tract_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data/Preliminary_2020_Census_Tract/Preliminary_2020_Census_Tract.shp</span><span class="sh">"</span>
<span class="n">tract_gdf</span> <span class="o">=</span> <span class="n">gpd</span><span class="p">.</span><span class="nf">read_file</span><span class="p">(</span><span class="n">census_tract_path</span><span class="p">)</span>

</code></pre></div></div> <p>As we can see here, the <code class="language-plaintext highlighter-rouge">POP10</code> field contains census counts.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tract_gdf</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>OBJECTID</th> <th>STATEFP</th> <th>COUNTYFP</th> <th>TRACTCE</th> <th>NAME</th> <th>TRACTID</th> <th>TRACTLABEL</th> <th>POP10</th> <th>HOUSING10</th> <th>SHAPEAREA</th> <th>SHAPELEN</th> <th>geometry</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>21</td> <td>11</td> <td>001</td> <td>001301</td> <td>13.01</td> <td>11001001301</td> <td>13.01</td> <td>3955</td> <td>2156</td> <td>2.882225e+06</td> <td>8705.698378</td> <td>POLYGON ((-77.06943 38.95434, -77.06932 38.954...</td> </tr> <tr> <th>1</th> <td>22</td> <td>11</td> <td>001</td> <td>002001</td> <td>20.01</td> <td>11001002001</td> <td>20.01</td> <td>2340</td> <td>1026</td> <td>6.337953e+05</td> <td>4198.601803</td> <td>POLYGON ((-77.04338 38.96146, -77.04329 38.961...</td> </tr> <tr> <th>2</th> <td>23</td> <td>11</td> <td>001</td> <td>003302</td> <td>33.02</td> <td>11001003302</td> <td>33.02</td> <td>2134</td> <td>982</td> <td>2.042153e+05</td> <td>1915.794576</td> <td>POLYGON ((-77.01428 38.91506, -77.01275 38.915...</td> </tr> <tr> <th>3</th> <td>24</td> <td>11</td> <td>001</td> <td>008402</td> <td>84.02</td> <td>11001008402</td> <td>84.02</td> <td>2149</td> <td>1270</td> <td>2.741538e+05</td> <td>2698.287213</td> <td>POLYGON ((-76.99497 38.89741, -76.99496 38.898...</td> </tr> <tr> <th>4</th> <td>1</td> <td>11</td> <td>001</td> <td>000101</td> <td>1.01</td> <td>11001000101</td> <td>1.01</td> <td>1384</td> <td>999</td> <td>1.993245e+05</td> <td>2168.618432</td> <td>POLYGON ((-77.05714 38.91055, -77.05702 38.910...</td> </tr> </tbody> </table> </div> <p>Our next step is to create a data table in which each row corresponds to a birth event and is associated with geospatial coordinates as well as a year.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">base_pregnancy_rate</span> <span class="o">=</span> <span class="mf">11.4</span> <span class="o">/</span> <span class="mi">1000</span> <span class="c1"># births per thousand people
</span><span class="n">years</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">2019</span><span class="p">)</span>

<span class="n">birth_coords</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">birth_points</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">year</span> <span class="ow">in</span> <span class="n">years</span><span class="p">:</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tract</span> <span class="ow">in</span> <span class="n">tract_gdf</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
        <span class="n">tract_boundary</span> <span class="o">=</span> <span class="n">tract</span><span class="p">.</span><span class="n">geometry</span>
        <span class="n">left</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="n">tract_boundary</span><span class="p">.</span><span class="n">bounds</span>
        <span class="n">n_pregnancies</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">base_pregnancy_rate</span> <span class="o">*</span> <span class="n">tract</span><span class="p">[</span><span class="sh">'</span><span class="s">POP10</span><span class="sh">'</span><span class="p">])</span>
        
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_pregnancies</span><span class="p">):</span>
            <span class="n">is_in_bounds</span> <span class="o">=</span> <span class="bp">False</span>
            
            <span class="k">while</span> <span class="ow">not</span> <span class="n">is_in_bounds</span><span class="p">:</span>
                <span class="n">coords</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="p">[</span><span class="n">left</span><span class="p">,</span><span class="n">lower</span><span class="p">],</span><span class="n">high</span><span class="o">=</span><span class="p">[</span><span class="n">right</span><span class="p">,</span><span class="n">upper</span><span class="p">])</span>
                <span class="n">sample</span> <span class="o">=</span> <span class="n">shapely</span><span class="p">.</span><span class="n">geometry</span><span class="p">.</span><span class="nc">Point</span><span class="p">(</span><span class="n">coords</span><span class="p">)</span>
                <span class="n">is_in_bounds</span> <span class="o">=</span> <span class="n">tract</span><span class="p">.</span><span class="n">geometry</span><span class="p">.</span><span class="nf">contains</span><span class="p">((</span><span class="n">sample</span><span class="p">))</span>
                
            <span class="n">birth_points</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
            <span class="n">birth_coords</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">coords</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="n">year</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">()])</span>
                     
<span class="n">birth_gdf</span> <span class="o">=</span> <span class="n">gpd</span><span class="p">.</span><span class="nc">GeoDataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">birth_coords</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">lat</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">lon</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">],</span> <span class="n">geometry</span><span class="o">=</span><span class="n">birth_points</span><span class="p">)</span>
<span class="n">birth_gdf</span><span class="p">[</span><span class="sh">"</span><span class="s">year_int</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">birth_gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">birth_gdf</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>lat</th> <th>lon</th> <th>year</th> <th>geometry</th> <th>year_int</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>-77.062823</td> <td>38.951295</td> <td>2010.816216</td> <td>POINT (-77.06282 38.95130)</td> <td>2010</td> </tr> <tr> <th>1</th> <td>-77.056945</td> <td>38.949016</td> <td>2010.555112</td> <td>POINT (-77.05695 38.94902)</td> <td>2010</td> </tr> <tr> <th>2</th> <td>-77.066707</td> <td>38.951606</td> <td>2010.082630</td> <td>POINT (-77.06671 38.95161)</td> <td>2010</td> </tr> <tr> <th>3</th> <td>-77.062031</td> <td>38.951623</td> <td>2010.639241</td> <td>POINT (-77.06203 38.95162)</td> <td>2010</td> </tr> <tr> <th>4</th> <td>-77.064879</td> <td>38.955598</td> <td>2010.950265</td> <td>POINT (-77.06488 38.95560)</td> <td>2010</td> </tr> </tbody> </table> </div> <p>To make this problem more interesting, we’ll simulate preterm births with spatial dependency. Our true generative process will allow for more preterm births in locations which are farther to the east and north.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">birth_df</span>   <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">birth_gdf</span><span class="p">).</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">geometry</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">year_int</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scales</span>     <span class="o">=</span> <span class="n">birth_df</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span>
<span class="n">means</span>      <span class="o">=</span>  <span class="n">birth_df</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
<span class="n">zscore_gdf</span> <span class="o">=</span> <span class="p">(</span><span class="n">birth_df</span> <span class="o">-</span><span class="n">means</span><span class="p">)</span><span class="o">/</span><span class="n">scales</span>

<span class="c1"># coefs are for lat, lon, and year respectively.
</span><span class="n">true_coefficients</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>

<span class="c1"># this value was chosen by hand to roughly line up with ~12% preterm births, on average
</span><span class="n">true_intercept</span>    <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">zscore_gdf</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">true_coefficients</span><span class="p">)</span><span class="o">+</span><span class="n">true_intercept</span>

<span class="n">birth_gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">preterm_prob</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

<span class="n">birth_gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">preterm</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">birth_gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">preterm_prob</span><span class="sh">'</span><span class="p">])</span>
<span class="n">birth_gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">preterm</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.12306756134464839
</code></pre></div></div> <p>Let’s see the spatial point pattern for the births. The preterm births are marked in red while normal births are marked with blue points.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">22</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">year</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">years</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]):</span>
    <span class="n">birth_gdf</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">year_int==</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s"> &amp; preterm==1</span><span class="sh">"</span><span class="p">).</span><span class="nf">plot</span><span class="p">(</span><span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                            <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">birth_gdf</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">year_int==</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s"> &amp; preterm==0</span><span class="sh">"</span><span class="p">).</span><span class="nf">plot</span><span class="p">(</span><span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                            <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    
    <span class="n">tract_gdf</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">facecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">preterm_frac</span> <span class="o">=</span> <span class="n">birth_gdf</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">year_int==</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="sh">"</span><span class="p">)[</span><span class="sh">'</span><span class="s">preterm</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Simulated births for </span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="se">\n</span><span class="s">(Preterm fraction: </span><span class="si">{</span><span class="nf">int</span><span class="p">(</span><span class="n">preterm_frac</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s">%)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>

</code></pre></div></div> <p><img src="/images/preterm20_0.png" alt="png" style="width:75%;"/></p> <p>A flaw of this simulation is that there are clearly jumps in point density at the interface between high- and low-population census tracts which are not reflective of reality.</p> <h2 id="preprocessing-spatial-adjacency-data">Preprocessing spatial adjacency data</h2> <p>Since we don’t want to construct our model directly at the point level, we instead need to aggregate to a larger spatial unit. For this purpose, we’ll use the <code class="language-plaintext highlighter-rouge">h3</code> library to aggregate into hexagonal bins.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h3_level</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">gdf</span> <span class="o">=</span> <span class="n">birth_gdf</span>
<span class="n">gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">]</span>      <span class="o">=</span> <span class="n">gdf</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">lat_lng_to_h3</span><span class="p">,</span> <span class="n">h3_level</span><span class="o">=</span><span class="n">h3_level</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">hexagon</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">gdf</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">add_geometry</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">hex_only</span> <span class="o">=</span> <span class="n">gdf</span><span class="p">[[</span><span class="sh">'</span><span class="s">hexagon</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">]].</span><span class="nf">drop_duplicates</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">)</span>
<span class="n">hex_only</span> <span class="o">=</span> <span class="n">gpd</span><span class="p">.</span><span class="nc">GeoDataFrame</span><span class="p">(</span><span class="n">geometry</span><span class="o">=</span><span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">hexagon</span><span class="sh">'</span><span class="p">],</span> <span class="n">data</span><span class="o">=</span><span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">])</span>
<span class="n">hex_only</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">h3_to_int</span> <span class="o">=</span> <span class="p">{</span><span class="n">code</span><span class="p">:</span> <span class="n">integer</span> <span class="k">for</span> <span class="n">integer</span><span class="p">,</span> <span class="n">code</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">()))}</span>
<span class="n">gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">h3_int</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">gdf</span><span class="p">[</span><span class="sh">'</span><span class="s">h3</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">h3_to_int</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">hex_only</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1703, 2)
</code></pre></div></div> <p>Under our model, each of the H3 cells is assumed to have its own free parameter for the probability of preterm birth. However, we will use the CAR prior to allow for pooling information across spatial cells and encouraging spatial smoothness in their estimates.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">hex_only</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">H3 spatial cells</span><span class="sh">'</span><span class="p">)</span>
<span class="n">tract_gdf</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">DC census tracts</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/images/preterm26_0.png" alt="png" style="width:75%;"/></p> <p>As a final preprocessing step, we need to create the adjacency matrix \(W\) and ensure that every node has a path through the adjacency matrix to every other path. Put more formally, we need to ensure there is only a single connected component in \(W\) and that it is nontrivial.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In geographic coordinate system
</span><span class="n">very_small_distance</span> <span class="o">=</span> <span class="mf">0.0003</span>
<span class="n">W</span> <span class="o">=</span> <span class="nf">adjacency_via_buffer</span><span class="p">(</span><span class="n">hex_only</span><span class="p">,</span> <span class="n">very_small_distance</span><span class="o">=</span><span class="n">very_small_distance</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="nf">connect_components</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">hex_only</span><span class="p">)</span>
</code></pre></div></div> <p>To check the correctness of our procedures, we can make sure that every cell has at least neighbor and that no cell has more than six neighbors</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">neighbors_per_cell</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">neighbors_per_cell</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;</span> <span class="n">neighbors_per_cell</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">6</span>
</code></pre></div></div> <h2 id="inference-for-model-parameters">Inference for model parameters</h2> <p>The probabilistic model we use has the following specification:</p> \[\alpha \sim Uniform(-0.95, 0.95)\\ c \sim Normal^{+}(0, 4)\\ \beta_0 \sim Normal(0, 9)\\ \mathbf{u}\sim CAR(W, \alpha)\\ y_j \sim Binomial(n_j, \sigma(u_j + \beta_0))\] <p>Here, \(Normal^{+}\) refers to the half-normal distribution with a mode at zero and almost all probability mass placed on the positive real line. Then, the CAR prior assumes that \(\mathbf{u}\) has a multivariate normal distribution with a spatially-smoothed covariance matrix. The spatial smoothing is informed by the cellwise adjacency matrix \(W\) and the spatial correlation parameter \(\alpha\). Finally, the number of preterm births within the \(i\)-th spatial cell is assumed to follow a binomial distribution with its logit specified as the spatial effect plus an intercept.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">preterm_counts</span> <span class="o">=</span> <span class="n">gdf</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">h3_int</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">preterm</span><span class="sh">'</span><span class="p">].</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">total_counts</span>   <span class="o">=</span> <span class="n">gdf</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">h3_int</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">preterm</span><span class="sh">'</span><span class="p">].</span><span class="nf">count</span><span class="p">()</span>

<span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">preterm_counts</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Hyperparameters on spatial correlation, random effect size, and model intercept
</span>    <span class="n">alpha</span>       <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Uniform</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span><span class="n">lower</span><span class="o">=-</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
    <span class="n">scale</span>       <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">intercept</span>   <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">intercept</span><span class="sh">'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="c1"># Spatially-smoothing prior on logit of preterm birth probability
</span>    <span class="n">spatial_effect</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">CAR</span><span class="p">(</span><span class="sh">'</span><span class="s">spatial_effect</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">likelihood</span>  <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">spatial_effect</span><span class="o">*</span><span class="n">scale</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">),</span>
                             <span class="n">observed</span><span class="o">=</span><span class="n">preterm_counts</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total_counts</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
    
    <span class="c1"># Applies Markov chain Monte Carlo to draw from the posterior distribution
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">target_accept</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;&lt;!! BUG IN FGRAPH.REPLACE OR A LISTENER !!&gt;&gt; &lt;class 'TypeError'&gt; Cannot convert Type TensorType(float64, matrix) (of Variable Usmm{no_inplace}.0) into Type TensorType(float64, row). You can try to manually convert Usmm{no_inplace}.0 into a TensorType(float64, row). Elemwise{sub,no_inplace}(z, Elemwise{mul,no_inplace}(alpha subject to &lt;function &lt;lambda&gt; at 0x7f5e61173c10&gt;, SparseDot(x, y))) -&gt; Usmm{no_inplace}(Elemwise{neg,no_inplace}(alpha), x, y, z)
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [spatial_effect, intercept, scale, alpha]
</code></pre></div></div> <div> <style>progress{border:0;background-size:auto}.progress-bar-interrupted,.progress-bar-interrupted::-webkit-progress-bar{background:#f44336}</style> <progress value="12000" class="" max="12000" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [12000/12000 02:01&lt;00:00 Sampling 4 chains, 0 divergences] </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sampling 4 chains for 2_000 tune and 1_000 draw iterations (8_000 + 4_000 draws total) took 122 seconds.
The number of effective samples is smaller than 10% for some parameters.
</code></pre></div></div> <p>Our posterior summary, as reported below, indicates strong evidence for spatial autocorrelation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">intercept</span><span class="sh">'</span><span class="p">]))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \
scale      0.483  0.036   0.411    0.547      0.001    0.001    1307.0   
alpha      0.947  0.003   0.943    0.950      0.000    0.000    3294.0   
intercept -1.983  0.026  -2.033   -1.936      0.001    0.001     363.0   

           ess_tail  r_hat  
scale        1876.0   1.00  
alpha        1618.0   1.00  
intercept     701.0   1.01  
</code></pre></div></div> <p>Our trace plots look good - no multimodality and the samples look uncorrelated. While the posterior for \(\alpha\) is piling up near the edge of the boundary, this is fine.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">intercept</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/images/preterm37_0.png" alt="png" style="width:75%;"/></p> <p>We next generate two plots to visualize the resulting estimates.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sigma_cutoff</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">estimated_intercept</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="sh">'</span><span class="s">intercept</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>

<span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">preterm_fraction</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">preterm_counts</span> <span class="o">/</span> <span class="n">total_counts</span><span class="p">).</span><span class="n">values</span>
<span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">estimate</span><span class="sh">'</span><span class="p">]</span>   <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="sh">'</span><span class="s">spatial_effect</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">stdevs</span><span class="sh">'</span><span class="p">]</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="sh">'</span><span class="s">spatial_effect</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">trace</span><span class="p">[</span><span class="sh">'</span><span class="s">spatial_effect</span><span class="sh">'</span><span class="p">].</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">is_sig</span><span class="sh">'</span><span class="p">]</span>     <span class="o">=</span> <span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">stdevs</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">sigma_cutoff</span>
<span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">delta_prob</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">hex_only</span><span class="p">[</span><span class="sh">'</span><span class="s">estimate</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="n">estimated_intercept</span><span class="p">)</span> <span class="o">-</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">estimated_intercept</span><span class="p">)</span>
</code></pre></div></div> <p>First, we create a plot of the data - the observed ratios of preterm births on a cell-by-cell basis.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">))</span>
<span class="n">hex_only</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="sh">'</span><span class="s">preterm_fraction</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">handle</span> <span class="o">=</span> <span class="n">tract_gdf</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>

<span class="n">row_ctr</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">hex_only</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">cent</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">geometry</span><span class="sh">'</span><span class="p">].</span><span class="n">centroid</span> 
    <span class="n">ax</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">cent</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">cent</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">preterm_counts</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row_ctr</span><span class="p">]</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">total_counts</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row_ctr</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span>
                 <span class="n">ha</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">row_ctr</span> <span class="o">+=</span> <span class="mi">1</span>
    

</code></pre></div></div> <p><img src="/images/preterm41_0.png" alt="png" style="width:75%;"/></p> <p>Next, we compare against our inferred estimates. Cells for which our estimate of the spatial effect is significant at the \(2\sigma\) level are highlighted with a star.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ax</span> <span class="o">=</span> <span class="n">hex_only</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="sh">'</span><span class="s">estimate</span><span class="sh">'</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">))</span>
<span class="n">tract_gdf</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">hex_only</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">cent</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">geometry</span><span class="sh">'</span><span class="p">].</span><span class="n">centroid</span> 
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">is_sig</span><span class="sh">'</span><span class="p">]:</span>
        <span class="n">sig_str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">*</span><span class="sh">'</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sig_str</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">delta_prob</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">cent</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">cent</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">sig_str</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Simulated preterm birth ratio</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Inferred change in probability of preterm birth</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/preterm43_0.png" alt="png" style="width:75%;"/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Conditional autoregression for 6-adjacent data]]></summary></entry><entry><title type="html">Surrogate modeling for SEIR dynamics</title><link href="https://ckrapu.github.io/blog/2021/creating-an-emulator-for-an-agent-based-model/" rel="alternate" type="text/html" title="Surrogate modeling for SEIR dynamics"/><published>2021-04-05T10:00:00+00:00</published><updated>2021-04-05T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2021/creating-an-emulator-for-an-agent-based-model</id><content type="html" xml:base="https://ckrapu.github.io/blog/2021/creating-an-emulator-for-an-agent-based-model/"><![CDATA[<p>Computers are (still) getting faster every year and it is now commonplace to run simulations in seconds that would have required hours’ or days’ worth of compute time in previous generations. That said, we still often come across cases where our computer models are simply too intricate and/or have too many components to run as often and quickly as we would like. In this scenario, we are frequently forced to choose a limited subset of potential scenarios manifest as parameter settings for which we have the resources to run simulations. I’ve written this notebook to show how to use a <em>statistical emulator</em> to help understand how the outputs of a model’s simulations might vary with parameters.</p> <p>This is going to be similar in many ways to the paper written by Kennedy and O’Hagan (2001) which is frequently cited on the subject, though our approach will be simpler in some regards.To start us off, I’ve modified an example of an agent-based model for disease spread on a grid which was written by Damien Farrell on <a href="https://dmnfarrell.github.io/bioinformatics/abm-mesa-python">his personal site</a>. We’re going to write a statistical emulator in PyMC3 and use it to infer likely values for the date of peak infection <em>without</em> running the simulator exhaustively over the entire parameter space.</p> <p><strong>TL;DR</strong>: we run our simulation for a few combinations of parameter settings and then try to estimate a simulation summary statistic for the entire parameter space.</p> <p>If you’re interested in reproducing this notebook, you can find the <code class="language-plaintext highlighter-rouge">abm_lib.py</code> file at <a href="https://gist.github.com/ckrapu/e2fb8692972ec2b499a1494760ff626e">this gist</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">abm_lib</span> <span class="kn">import</span> <span class="n">SIR</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="sh">'</span><span class="s">retina</span><span class="sh">'</span>
</code></pre></div></div> <h2 id="simulating-with-an-abm">Simulating with an ABM</h2> <p>We’ll first need to specify the parameters for the SIR model. This model is fairly rudimentary and is parameterized by:</p> <ul> <li>The number of agents in the simulation</li> <li>The height and width of the spatial grid</li> <li>The proportion of infected agents at the beginng</li> <li>Probability of infecting other agents in the same grid cell</li> <li>Probability of dying from the infection</li> <li>Mean + standard deviation of time required to overcome the infection and recover</li> </ul> <p>These parameters, as well as the number of timesteps in the simulation, are all specified in the following cells. I am going to let most of the parameters be fixed as single values - only two parameters will be allowed to vary in our simulations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fixed_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">:</span><span class="mi">20000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">width</span><span class="sh">"</span><span class="p">:</span><span class="mi">80</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">height</span><span class="sh">"</span><span class="p">:</span><span class="mi">30</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">recovery_sd</span><span class="sh">"</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">recovery_days</span><span class="sh">"</span><span class="p">:</span><span class="mi">21</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">p_infected_initial</span><span class="sh">"</span><span class="p">:</span><span class="mf">0.0002</span>
<span class="p">}</span>
</code></pre></div></div> <p>For the probability of transmission and death rate, we’ll randomly sample some values from the domains indicated below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_bounds</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">ptrans</span><span class="sh">"</span><span class="p">:[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">death_rate</span><span class="sh">"</span><span class="p">:[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
<span class="p">}</span>
</code></pre></div></div> <p>Here, we iteratively sample new values of the parameters and run the simulation. Since each one takes ~40 seconds, it would take too long to run the simulation at every single parameter value in a dense grid of 1000 or more possible settings.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">n_samples_init</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">input_dicts</span>    <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_samples_init</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">fixed_params</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">sample_bounds</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">*</span><span class="n">v</span><span class="p">)</span>
    <span class="n">input_dicts</span> <span class="o">+=</span> <span class="p">[</span><span class="n">d</span><span class="p">]</span>
    

<span class="n">n_steps</span><span class="o">=</span><span class="mi">100</span>

<span class="n">simulations</span> <span class="o">=</span> <span class="p">[</span><span class="nc">SIR</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">input_dicts</span><span class="p">]</span>
<span class="n">all_states</span>  <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">simulations</span><span class="p">]</span>


</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:57&lt;00:00,  1.75it/s]
100%|██████████| 100/100 [00:36&lt;00:00,  2.78it/s]
100%|██████████| 100/100 [00:52&lt;00:00,  1.91it/s]
100%|██████████| 100/100 [00:35&lt;00:00,  2.80it/s]
100%|██████████| 100/100 [00:37&lt;00:00,  2.64it/s]
100%|██████████| 100/100 [00:40&lt;00:00,  2.48it/s]
100%|██████████| 100/100 [00:40&lt;00:00,  2.47it/s]
100%|██████████| 100/100 [00:40&lt;00:00,  2.44it/s]
100%|██████████| 100/100 [00:39&lt;00:00,  2.50it/s]
100%|██████████| 100/100 [00:23&lt;00:00,  4.32it/s]
</code></pre></div></div> <p>Next, we combine all the sampled parameter values into a dataframe. We also add a column for our response variable which presents a summary of the results from the ABM simulation. We’ll use the timestep for which the level of infection was highest as the <code class="language-plaintext highlighter-rouge">worst_day</code> column in the dataframe.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">input_dicts</span><span class="p">)</span>

<span class="c1"># Add column showing the day with the peak infection rate
</span><span class="n">params_df</span><span class="p">[</span><span class="sh">'</span><span class="s">worst_day</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">[...,</span><span class="mi">1</span><span class="p">].</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_states</span><span class="p">]</span>
</code></pre></div></div> <p>We can also spit out a few animations to visualize how the model dynamics behave. This can take quite awhile, however.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.gridspec</span> <span class="k">as</span> <span class="n">gridspec</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">mpl_toolkits.axes_grid1</span> <span class="kn">import</span> <span class="n">make_axes_locatable</span>

<span class="n">generate_animations</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">figure_directory</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./figures/sir-states/</span><span class="sh">'</span>

<span class="k">if</span> <span class="n">generate_animations</span><span class="p">:</span>
    <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">simulations</span><span class="p">):</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">pair</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)):</span>
            <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">constrained_layout</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="p">.</span><span class="nc">GridSpec</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figure</span><span class="o">=</span><span class="n">fig</span><span class="p">)</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">])</span>
            <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:,</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">jet</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">set_axis_off</span><span class="p">()</span>
            <span class="n">divider</span> <span class="o">=</span> <span class="nf">make_axes_locatable</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
            <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="p">.</span><span class="nf">append_axes</span><span class="p">(</span><span class="sh">"</span><span class="s">right</span><span class="sh">"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="sh">"</span><span class="s">3%</span><span class="sh">"</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Number infected</span><span class="sh">'</span><span class="p">)</span>

            <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
                <span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">state</span><span class="p">[:,:,:,</span><span class="n">j</span><span class="p">].</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
                <span class="n">ax2</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:,</span><span class="n">j</span><span class="p">].</span><span class="nf">sum</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            <span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number infected</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">ax2</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Timestep</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">figure_directory</span><span class="o">+</span><span class="sh">'</span><span class="s">frame_{1}_{0}.jpg</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="nf">zfill</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span><span class="n">j</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">'</span><span class="s">tight</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">250</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

        <span class="err">!</span> <span class="n">cd</span> <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">v7k</span><span class="o">/</span><span class="n">Dropbox</span>\ \<span class="p">(</span><span class="n">ORNL</span>\<span class="p">)</span><span class="o">/</span><span class="n">research</span><span class="o">/</span><span class="n">abm</span><span class="o">-</span><span class="n">inference</span><span class="o">/</span><span class="n">figures</span><span class="o">/</span><span class="n">sir</span><span class="o">-</span><span class="n">states</span><span class="o">/</span><span class="p">;</span> <span class="n">convert</span> <span class="o">*</span><span class="p">.</span><span class="n">jpg</span> <span class="n">sir_states</span><span class="p">{</span><span class="n">k</span><span class="p">}.</span><span class="n">gif</span><span class="p">;</span> <span class="n">rm</span> <span class="o">*</span><span class="p">.</span><span class="n">jpg</span>
</code></pre></div></div> <p><img src="/images/sir_states9.gif" alt="gif"/> <img src="/images/sir_states.gif" alt="gif"/></p> <p>Clearly, the model parameters make a major difference in the rate of spread of the virus. In the lower case, the spread requires over 100 timesteps to infect most of the agents.</p> <p>If we make a plot depicting the date of peak infection as a function of <code class="language-plaintext highlighter-rouge">ptrans</code> and <code class="language-plaintext highlighter-rouge">death_rate</code>, we’ll get something that looks like the picture below. This is a fairly small set of points and the rest of this notebook will focus on interpolating between them in a way which provides quantified uncertainty.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">params_df</span><span class="p">.</span><span class="n">death_rate</span><span class="p">,</span> <span class="n">params_df</span><span class="p">.</span><span class="n">ptrans</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">params_df</span><span class="p">.</span><span class="n">worst_day</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Death rate</span><span class="sh">'</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Transmission probability</span><span class="sh">'</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Day / timestep</span><span class="sh">"</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/2021-04-05-spatial-abm-emulator_19_0.png" alt="png"/></p> <h2 id="building-a-simplified-gaussian-process-emulator">Building a simplified Gaussian process emulator</h2> <p>Our probabilistic model for interpolating between ABM parameter points is shown below in the next few code cells. We first rescale the parameter points and the response variable to have unit variance. This makes it a little easier to specify reasonable priors for the parameters of our Gaussian process model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">param_scales</span>  <span class="o">=</span> <span class="n">params_df</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span>
<span class="n">params_df_std</span> <span class="o">=</span> <span class="n">params_df</span> <span class="o">/</span> <span class="n">param_scales</span>

<span class="n">input_vars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">sample_bounds</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
<span class="n">n_inputs</span>   <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">input_vars</span><span class="p">)</span>
</code></pre></div></div> <p>We assume that the mean function of our Gaussian process is a constant, and we use fairly standard priors for the remaining GP parameters. In particular, we use a <code class="language-plaintext highlighter-rouge">Matern52</code> covariance kernel which allows the correlation between values of our response variable to be a function of the Euclidean distance between them.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pymc3</span> <span class="k">as</span> <span class="n">pm</span>

<span class="k">def</span> <span class="nf">sample_emulator_model_basic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sampler_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">target_accept</span><span class="sh">'</span><span class="p">:</span><span class="mf">0.95</span><span class="p">}):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    
    <span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">emulator_model</span><span class="p">:</span>
        <span class="n">intercept</span>    <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">intercept</span><span class="sh">'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
        <span class="n">length_scale</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">length_scale</span><span class="sh">'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">variance</span>     <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">InverseGamma</span><span class="p">(</span><span class="sh">'</span><span class="s">variance</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="n">cov_func</span>     <span class="o">=</span> <span class="n">variance</span><span class="o">*</span><span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="nc">Matern52</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="n">length_scale</span><span class="p">)</span>
        <span class="n">mean_func</span>    <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">mean</span><span class="p">.</span><span class="nc">Constant</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>
        
        <span class="n">gp</span>       <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="nc">Marginal</span><span class="p">(</span><span class="n">mean_func</span><span class="o">=</span><span class="n">mean_func</span><span class="p">,</span> <span class="n">cov_func</span><span class="o">=</span><span class="n">cov_func</span><span class="p">)</span>
        <span class="n">noise</span>    <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">noise</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="nf">marginal_likelihood</span><span class="p">(</span><span class="sh">'</span><span class="s">response</span><span class="sh">'</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
        <span class="n">trace</span>    <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="o">**</span><span class="n">sampler_kwargs</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">trace</span><span class="p">,</span> <span class="n">emulator_model</span><span class="p">,</span> <span class="n">gp</span>
</code></pre></div></div> <p>Fitting the model runs fairly quickly since we have only a handful of observed data points. If we had 1000 or more instead of 10, we might need to use a different flavor of Gaussian process model to accommodate the larger set of data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">params_df_std</span><span class="p">[</span><span class="n">input_vars</span><span class="p">].</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">params_df_std</span><span class="p">[</span><span class="sh">'</span><span class="s">worst_day</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>

<span class="n">trace</span><span class="p">,</span> <span class="n">emulator_model</span><span class="p">,</span> <span class="n">gp</span> <span class="o">=</span> <span class="nf">sample_emulator_model_basic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;ipython-input-15-996f43c1af4e&gt;:17: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.
  trace    = pm.sample(**sampler_kwargs)
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [noise, variance, length_scale, intercept]
</code></pre></div></div> <div> <style>progress{border:0;background-size:auto}.progress-bar-interrupted,.progress-bar-interrupted::-webkit-progress-bar{background:#f44336}</style> <progress value="8000" class="" max="8000" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [8000/8000 00:29&lt;00:00 Sampling 4 chains, 0 divergences] </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 30 seconds.
The number of effective samples is smaller than 25% for some parameters.
</code></pre></div></div> <p>Predicting at new locations is easy too, once we have our fitted model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xnew</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">sample_bounds</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">input_vars</span><span class="p">]))</span>
<span class="n">Xnew</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="n">Xnew</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">Xnew</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">ravel</span><span class="p">()]).</span><span class="n">T</span> <span class="o">/</span> <span class="n">param_scales</span><span class="p">[</span><span class="n">input_vars</span><span class="p">].</span><span class="n">values</span>

<span class="k">with</span> <span class="n">emulator_model</span><span class="p">:</span>
    <span class="n">pred_mean</span><span class="p">,</span> <span class="n">pred_var</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="n">given</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>The final two cells create plots showing the posterior predictive distribution of the GP over all the values in parameter space for which we have no data. As we can see, it smoothly interpolates between data points.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">pred_mean</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Death rate</span><span class="sh">'</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Transmission probability</span><span class="sh">'</span><span class="p">),</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Posterior mean</span><span class="sh">'</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Posterior mean surface</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;matplotlib.colorbar.Colorbar at 0x7fd2c3beb3a0&gt;,
 Text(0.5, 1.0, 'Posterior mean surface'))
</code></pre></div></div> <p><img src="/images/2021-04-05-spatial-abm-emulator_30_1.png" alt="png"/></p> <p>We also see that the variance in the predictions grows as we move farther and farther away from observed data points.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">pred_var</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Death rate</span><span class="sh">'</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Transmission probability</span><span class="sh">'</span><span class="p">),</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Posterior variance</span><span class="sh">'</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Posterior variance surface</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/2021-04-05-spatial-abm-emulator_32_0.png" alt="png"/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Modeling a model, for epidemiology]]></summary></entry><entry><title type="html">Density estimation for geospatial imagery using autoregressive neural models</title><link href="https://ckrapu.github.io/blog/2020/density-estimation-for-geospatial-imagery-using-autoregressive-models/" rel="alternate" type="text/html" title="Density estimation for geospatial imagery using autoregressive neural models"/><published>2020-03-30T10:00:00+00:00</published><updated>2020-03-30T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2020/density-estimation-for-geospatial-imagery-using-autoregressive-models</id><content type="html" xml:base="https://ckrapu.github.io/blog/2020/density-estimation-for-geospatial-imagery-using-autoregressive-models/"><![CDATA[<p>Bayesian machine learning is all about learning a good representation of very complicated datasets, leveraging cleverly structured models and effective parameter estimation techniques to create a high-dimensional probability distribution approximating the observed data. A key advantage of posing computer vision research under the umbrella of Bayesian inference is that some tasks become really straightforward with the right choice of model.</p> <p>In this notebook, I show how to use <strong>PixelCNN</strong>, a deep generative model of structured data, to perform density estimation on geospatial topographic imagery derived from LiDAR maps of the Earth’s surface. I also highlight how easy this is within TensorFlow Probability, a new open-source project extending the capabilities of Tensorflow into <strong>probabilistic programming</strong>, i.e. the representation of probability distributions with computer programs in a way that treats random variables as first-class citizens.</p> <p><strong>Note</strong>: To reproduce this notebook, you will need the digital elevation map dataset I used to train the model. It’s too large to be hosted on my Github repository. Email me at ckrapu at gmail.com to get everything you need to reproduce this!</p> <h3 id="density-estimation">Density estimation</h3> <p>Density estimation is a task which has a common sense interpretation: if our understanding of the world is encoded in a probabilistic model, data points with especially low density are <strong>rare</strong> according to the model while points with high density are <strong>common</strong>. Suppose that you are walking down the street and you see a bright, neon blue dog that is as large as a firetruck. This is an instance which would probably receive low density under your subjective model of the world because there is exceedingly low probability of it appearing. Conversely, a smaller brown dog would receive a higher density value because it is more likely under the set of beliefs and assumptions you hold about the world.</p> <p>Most probability distributions are not as rich or flexible as the set of beliefs that we individually hold about the world. Coming up with extremely flexible and rich distributions is an active area of research. As of right now, a leading approach to generating these distributions is via neural autoregressive models which extend standard time series models such as the autoregressive or ARIMA models to have a neural transition operation rather than a linear, Markovian operation. The <a href="https://arxiv.org/abs/1606.05328">PixelCNN architecture</a> is a popular neural autoregressive model currently in use.</p> <p>Many machine learning models of imagery do not allow for easy density estimation. For example, the variational autoencoder provides a mapping from latent variable \(\mathbf{z}\) to observed data point \(\mathbf{x}\). Unfortunately, calculating \(p(\mathbf{x})\) under the model typically requires approximating the integral \(p(\mathbf{x}) = \int_z p(\mathbf{x}\vert \mathbf{z})p(\mathbf{z}) d\mathbf{z}\). Autoregressive models, in their most basic form, just don’t have this latent variable representative and instead parameterize the function \(p(x_i \vert x_{i-1},...,x_1)\) where \(x_i\) denotes the \(i\)-th pixel in the image. This admits a decomposition of the image’s probability as \(p(\mathbf{x})=\prod_i p(x_i\vert x_{i-1},...,x_1)\). This assumes a total ordering of the pixels in an image; we usually assume the raster scan order (though there are <a href="https://arxiv.org/abs/1712.09763">creative solutions</a> which can improve on this!).</p> <p>The rest of this post shows how to use the PixelCNN distribution from Tensorflow Probability and apply density estimation. The PixelCNN distribution was included with the 0.9 update of <code class="language-plaintext highlighter-rouge">tensorflow-probability</code>, so you’ll need to upgrade your installation if you were on 0.8 or earlier.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="n">tensorflow_probability</span> <span class="k">as</span> <span class="n">tfp</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">flatten_image_batch</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">This script uses Tensorflow </span><span class="si">{</span><span class="n">tf</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Tensorflow Probability version: </span><span class="si">{</span><span class="n">tfp</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>This script uses Tensorflow 2.1.0
Tensorflow Probability version: 0.9.0
</code></pre></div></div> <p>The dataset that I’m using consists of images with dimension \(32\times32\times1\) representing topographical maps of the Earth’s surface in the state of North Dakota. Each pixel’s single channel of data represents the average elevation across several square meters. Features like roads, ditches, rivers and valleys can be seen in these images.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">../data/datasets/training/dem_32_filtered.npy</span><span class="sh">'</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dem_as_int</span> <span class="o">=</span> <span class="p">(((</span><span class="n">data_numpy</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">255</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
</code></pre></div></div> <p>Currently, the available architectures for PixelCNN work best when the output data is quantized. The image data originally had pixel values within the rage \([-1,1]\) which need to be mapped to \(\{0,1,...,255\}\). Let’s take a look below and see what these images look like:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">selected</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">data_numpy</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">size</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">dem_as_int</span><span class="p">[</span><span class="n">selected</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">32</span><span class="p">]</span>
<span class="n">flat</span> <span class="o">=</span> <span class="nf">flatten_image_batch</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">flat</span><span class="p">),</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Training data</span><span class="sh">'</span><span class="p">),</span><span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/density-estimation-for-geospatial-imagery-using-autoregressive-models_files/density-estimation-for-geospatial-imagery-using-autoregressive-models_5_0.png" alt="png"/></p> <p>Many of the images are of gently sloped or rolling surfaces with a few linear features such as ditches or roads. Many of the images have local regions of high variance corresponding to marshy vegetation which scatters the LiDAR pulses used for elevation estimation.</p> <p>The PixelCNN model is actually a joint distribution over all the pixels of an image. Thus, it was possible for the developers of the <code class="language-plaintext highlighter-rouge">tensorflow-probability</code> package to actually include it as one of their distributions! This makes it really easy to work with and the code below shows how little setup is required to train a PixelCNN with TFP. Much of this code was copied from the TFP documentation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Specify inputs and training settings
</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">filters</span> <span class="o">=</span> <span class="mi">96</span>

<span class="c1"># Create a Tensorflow Dataset object
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">(</span><span class="n">dem_as_int</span><span class="p">)</span>
<span class="n">train_it</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">).</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">data_numpy</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Create the PixelCNN using TFP
</span><span class="n">dist</span> <span class="o">=</span> <span class="n">tfp</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">PixelCNN</span><span class="p">(</span>
    <span class="n">image_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
    <span class="n">num_resnet</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_hierarchies</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
    <span class="n">num_logistic_mix</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">dropout_p</span><span class="o">=</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define the model input and objective function
</span><span class="n">image_input</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">log_prob</span><span class="p">(</span><span class="n">image_input</span><span class="p">)</span>

<span class="c1"># Specify model inputs and loss function
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">image_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">log_prob</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add_loss</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">log_prob</span><span class="p">))</span>
</code></pre></div></div> <p>Once the model is specified, we just need to compile it and start training. PixelCNN is an example of an autoregressive model and these are notorious for taking a long time to train. Unfortunately, I only have access to a single GPU currently. Normally, this code would display a progress bar and training metrics. I’ve toggled these off to keep the document short and prevent a large number of warnings from being shown.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compile and train the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(.</span><span class="mi">001</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[])</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_it</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARNING:tensorflow:Output tf_op_layer_Reshape_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_Reshape_3.
Train for 4602 steps
Epoch 1/3
3626/4602 [======================&gt;.......] - ETA: 30:48 - loss: 2357.9549

IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)



4602/4602 [==============================] - 8735s 2s/step - loss: 1989.1206
Epoch 3/3
 612/4602 [==&gt;...........................] - ETA: 2:06:25 - loss: 1972.0003
</code></pre></div></div> <p>Since we’ve created an approximation of a probability distribution, we can sample from it to see examples of points that have high density under the PixelCNN model. As a warning, this sampling procedure can take quite awhile.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">36</span><span class="p">)</span>
</code></pre></div></div> <p>Let’s visually compare the sampled values with ground truth data points.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">utils</span> <span class="kn">import</span> <span class="n">flatten_image_batch</span>

<span class="n">samples_numpy</span> <span class="o">=</span> <span class="n">samples</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">squeeze</span><span class="p">()</span>
<span class="n">flat_samples</span> <span class="o">=</span> <span class="nf">flatten_image_batch</span><span class="p">(</span><span class="n">samples_numpy</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)),</span><span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">flat_samples</span><span class="p">),</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Simulated images</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

<span class="n">selected</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">data_numpy</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">size</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">flat_ground_truth</span> <span class="o">=</span> <span class="nf">flatten_image_batch</span><span class="p">(</span><span class="n">data_numpy</span><span class="p">[</span><span class="n">selected</span><span class="p">].</span><span class="nf">squeeze</span><span class="p">(),</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)),</span><span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">flat_ground_truth</span><span class="p">),</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">True images</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/images/density-estimation-for-geospatial-imagery-using-autoregressive-models_files/density-estimation-for-geospatial-imagery-using-autoregressive-models_14_0.png" alt="png"/></p> <p><img src="/images/density-estimation-for-geospatial-imagery-using-autoregressive-models_files/density-estimation-for-geospatial-imagery-using-autoregressive-models_14_1.png" alt="png"/></p> <p>Both the ground truth and sampled images appear to show winding streams and sloping hillsides, though there are more linear features such as roads and ditches in the true data than the synthetic samples.</p> <p>In the next cell, I calculate the log density of 1000 ground truth images using the PixelCNN as my probability distribution. I also calculate the ranking of each image with regard to its probability.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subset</span>    <span class="o">=</span> <span class="n">dem_as_int</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">log_prob</span><span class="p">(</span><span class="n">subset</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">ranking</span>   <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>

<span class="n">sorted_log_prob</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="n">ranking</span><span class="p">]</span>
</code></pre></div></div> <p>With these rankings, I can show images which have low, medium, or high density under the PixelCNN model</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span><span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">16</span><span class="p">))</span>
<span class="n">sorted_by_prob</span> <span class="o">=</span> <span class="n">subset</span><span class="p">[</span><span class="n">ranking</span><span class="p">]</span>

<span class="n">subsets</span> <span class="o">=</span> <span class="p">[</span><span class="n">sorted_by_prob</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">32</span><span class="p">],</span><span class="n">sorted_by_prob</span><span class="p">[</span><span class="mi">484</span><span class="p">:</span><span class="mi">516</span><span class="p">],</span><span class="n">sorted_by_prob</span><span class="p">[</span><span class="o">-</span><span class="mi">32</span><span class="p">:]]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Images with low density</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Images with medium density</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Images with high density</span><span class="sh">'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">subset</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">subsets</span><span class="p">):</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="nf">flatten_image_batch</span><span class="p">(</span><span class="n">subset</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">flat</span><span class="p">),</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p><img src="/images/density-estimation-for-geospatial-imagery-using-autoregressive-models_files/density-estimation-for-geospatial-imagery-using-autoregressive-models_19_0.png" alt="png"/></p> <p>These images help us understand the representation that the model has learned. In the top panel, we see that the images with the lowest probability are those with a lot of “fuzziness”; these are images with lots of noisy LiDAR reflections due to water and vegetation. Since this is effectively random noise, it isn’t possible to predict perfectly what these values will be.</p> <p>Images with high density, on the other hand, show smoothly varying topography and very strong spatial autocorrelations. Again, this isn’t terribly surprising because the model has favored data points for which it can easily yield very good pixel-level predictions. If each pixel differs from its neighbor by only a small amount, it is much easier to construct a predictive model with low error.</p> <p>I hope that this provided a straightforward and minimal example of how to use Tensorflow Probability for a rather sophisticated machine learning task. I’ve been impressed with the functionality incorporated into the TFP codebase and look forward to using it more in the future!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Conditional autoregression for 6-adjacent data]]></summary></entry><entry><title type="html">Multivariate sample size for Markov chains</title><link href="https://ckrapu.github.io/blog/2020/multivariate-sample-size-for-markov-chains/" rel="alternate" type="text/html" title="Multivariate sample size for Markov chains"/><published>2020-02-18T10:00:00+00:00</published><updated>2020-02-18T10:00:00+00:00</updated><id>https://ckrapu.github.io/blog/2020/multivariate-sample-size-for-markov-chains</id><content type="html" xml:base="https://ckrapu.github.io/blog/2020/multivariate-sample-size-for-markov-chains/"><![CDATA[<table> <tbody> <tr> <td><strong>Summary: I show how to calculate a multivariate effective sample size after <a href="https://academic.oup.com/biomet/article/106/2/321/5426969">Vats et al. (2019)</a></strong>. In applied statistics, Markov chain Monte Carlo (MCMC) is now widely used to fit statistical models. Suppose we have a statistical model \(p_\theta\) of some dataset \(\mathcal{D}\) which has a parameter \(\theta\). The basic idea behind MCMC is to estimate \(\theta\) by generating \(N\) random variates \(\theta_i,\theta_2,...\) from the posterior distribution $$p(\theta</td> <td>\mathcal{D})\(which are (hopefully) distributed around\)\theta\(in a predictable way. The Bayesian central limit theorem states that under the right conditions,\)\theta_i\(is normally distributed about the true parameter value\)\theta\(with some sample variance\)\sigma^2\(. We might then want to use the mean of these samples\)\hat{\theta}=\sum_i^N \theta_i\(as an estimator of\)\theta$$ since this <em>posterior mean</em> is an optimal estimator in the context of Bayes risk and mean square loss.</td> </tr> </tbody> </table> <p>This task has a few challenges lurking within. The accuracy of our estimate of \(\theta\) is going to be low when we have only a few samples, i.e. \(N\) is quite small. We can increase our accuracy by taking more samples. Ideally, our samples \(\theta_i\) are all going to be <em>independent</em> so that we can make use of the theory of Monte Carlo estimators to assert that the error in our estimation of \(\theta\) decreases at a rate of \(1/N\). Thus, to get more accuracy, we draw more samples!</p> <h2 id="autocorrelated-mcmc-draws">Autocorrelated MCMC draws</h2> <p>Unfortunately, MCMC won’t provide uncorrelated values of \(\theta_i\) because of its inherently sequential nature. These samples are going to have some autocorrelation \(\rho\) and it’s helpful to think of this autocorrelation as reduced the number of samples from a nominal \(N\) to an effective number \(N\). Here’s a helpful analogy - suppose that you want to determine the average income within a city. You could pursue two sampling strategies; the first leads you to travel to 10 spots randomly selected on the map and then query a single person. The second approach is that you travel to two neighborhoods and query five people each. The latter method has the downside that you may get grossly misrepresentative numbers if you happen to land in a neighborhood where everyone has similar incomes which are not close to the city-wide average. This is an example of <em>spatial</em> autocorrelation leading to poor estimation. The same underlying mechanism is at play with our MCMC estimator having reduced precision.</p> <p>The literature on sequential data makes frequent use to autocorrelations \(\rho_p\) of lag \(p\) meant to capture associations between data points with varying amounts of time or distance between them. We can provide a formula for the effective sample size in terms of these autocorrelations <a href="https://mc-stan.org/docs/2_22/reference-manual/effective-sample-size-section.html">(see here for more)</a> via the following formula: \(N_{eff} = \frac{N}{\sum_{t=-\infty}^{\infty}\rho_t}\)</p> <p>We truncate the sum in practice since the autocorrelations typically vanish after a large number of lags. Interestingly, the effective sample size also has another form which also implicitly involves autocorrelations. Suppose that we have a chain of samples \(\theta_1,...,\theta_N\) and partition this chain into two <em>batches</em> comprising the samples from \(1\) to \(N/2\) and from \(N/2\) to \(N\). If the samples are close to independent, then the per-batch means \(T_{(k)}\) should be relatively close to each other. If they aren’t, then the batches contain distinct subpopulations of samples. The key insight here is that if the subpopulations are distinct, then they exhibit high within-batch autocorrelation. Thus, we can attempt to back out the autocorrelations by looking at the differences between batch means! For \(a\) batch means, each of size \(N/a\), this produces the following quantity:</p> \[\lambda^2=\frac{N}{a(a-1)}\sum_k (T_{(k)}-\hat{\theta})^2\] <p>Then, if we take the ratio of this quantity with the overall sample variance \(\sigma^2\), we get another formula for the effective sample size:</p> \[N_{eff} = \frac{n\lambda^2}{\sigma^2}\] <h2 id="effective-sample-size-for-multivariate-draws">Effective sample size for multivariate draws</h2> <p>The aforementioned equations for the effective sample size are fine for draws of univariate quantities. We also want to know how to obtain an analogous number for vector-valued random processes. Researchers often attempt to do so by simply evaluating the scalar \(N_{eff}\) for each individual dimension of a chain of vector samples, but this isn’t very satisying. Fortunately, recent work by <a href="https://academic.oup.com/biomet/article/106/2/321/5426969">Dats et al. (2019)</a> has shown that a the straightforward multivariate generalization of the above formula works perfectly well! We simply have to generalize the quantities \(\lambda^2\) and \(\sigma^2\) to their matrix counterparts: \(\Lambda=\frac{N}{a(a-1)}\sum_k ({\vec{T}}_{(k)}-{\hat{\theta}})^T({\vec{T}}_{(k)}-\hat)\) \(\Sigma=\frac{1}{N-1}\sum_i ({\vec{\theta_i}}-{\hat{\theta}})^T({\vec{\theta_i}}^{(k)}-\hat)\) With these quantities, we write out the effective number of samples as before just with the matrix generalizations of all quantities involved. Note that here, \(p\) represents the dimension of \(\theta_i\).</p> \[N_{eff}^{multi} = N\left(\frac{|\Lambda|}{|\Sigma|}\right)^{1/p}\] <p>Note that you still need to choose how many batches are used - a rule-of-thumb (there are more technical conditions that are worth reading about, though) is to use a batch size of \(\sqrt{N}\), so if you have 256 samples then there would be 16 batches of 16 samples each.</p> <p>In the code below, I’ll show how to calculate this for a toy example.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">n</span>    <span class="o">=</span> <span class="mi">256</span> <span class="c1"># Number of draws
</span><span class="n">p</span>    <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># Dimension of each draw
</span><span class="n">cov</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="c1"># True covariance matrix
</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="c1"># true mean vector
</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">cov</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">n_batches</span>         <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">samples_per_batch</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">n</span> <span class="o">/</span> <span class="n">n_batches</span><span class="p">)</span>

<span class="c1"># Split up data into batches and take averages over
# individual batches as well as the whole dataset
</span><span class="n">batches</span>     <span class="o">=</span> <span class="n">samples</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">n_batches</span><span class="p">,</span><span class="n">samples_per_batch</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>
<span class="n">batch_means</span> <span class="o">=</span> <span class="n">batches</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">full_mean</span>   <span class="o">=</span> <span class="n">samples</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Calculate the matrix lam as a sum of vector
# outer products
</span><span class="n">prefactor</span>       <span class="o">=</span> <span class="n">samples_per_batch</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_batches</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">batch_residuals</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_means</span> <span class="o">-</span> <span class="n">full_mean</span><span class="p">)</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mi">0</span> 
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
    <span class="n">lam</span> <span class="o">+=</span> <span class="n">prefactor</span> <span class="o">*</span> <span class="p">(</span><span class="n">batch_residuals</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">batch_residuals</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,:].</span><span class="n">T</span><span class="p">)</span>

<span class="n">sigma</span> <span class="o">=</span>  <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">samples</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">n_eff</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">det</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">p</span><span class="p">)</span>

</code></pre></div></div> <p>Let’s see what \(N_{eff}\) is for this case:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">There are {0} effective samples</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">n_eff</span><span class="p">)))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>There are 166 effective samples
</code></pre></div></div> <p>Since I used 256 truly independent samples in total, it appears that this statistic is somewhat conservative in reporting the effective sample size. I hope this was useful! Again, you can read more about this method at <a href="https://academic.oup.com/biomet/article/106/2/321/5426969">this Biometrika article</a> by Dootika Vats et al.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Assessing effective sample size for multiple variates]]></summary></entry></feed>